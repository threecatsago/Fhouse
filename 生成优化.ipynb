{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "可选参数——精细地控制AI模型的输出,以获得最符合需求的结果\n",
    "\n",
    "通过合理设置这些参数,可以更好地控制LLM的输出,使其更符合特定任务的需求。例如,对于需要精确答案的任务,可以降低temperature值;对于需要创意输出的任务,可以提高temperature值。同时,通过设置适当的system提示,可以让LLM更好地理解任务上下文,从而提供更相关的回答等等。"
   ],
   "id": "5253cbace7a55967"
  },
  {
   "cell_type": "markdown",
   "source": [
    "一些参数既可以在制造特定模型的机器人（实例化）的时候设定，也可以在使用机器人（调用函数）的时候调整。\n",
    "如果你希望特定模型的机器人机器人一直按照某种方式工作，你可以在制造它的时候就设定好。—— genai.GenerativeModel 构造函数\n",
    "如果你只是偶尔需要改变机器人的工作方式，你可以在使用它的时候再进行调整。—— 每个请求中传递给 GenerativeModel.generate_content 或 ChatSession.send_message\n",
    "\n",
    "\n",
    "\n",
    "| 函数              | 必需参数               | 可选参数                                                                                            | 主要区别                  | 适用场景                       |\n",
    "|-------------------|-----------------------|-------------------------------------------------------------------------------------------------|-----------------------|----------------------------|\n",
    "| `init`            | `model_name`          | `generation_config`, `safety_settings`, `tools`, `tool_config`, **`system_instruction`**        | 用于初始化对象并设置基本参数        | 创建模型实例时设置基础配置              |\n",
    "| `generate_content`| `contents`            | `generation_config`, `safety_settings`,  `tools`, `tool_config`,`stream`, **`request_options`** | 主要用于生成内容，支持流式输出，原生的函数 | 模型实例生成内容的通用函数，也可自己添加历史消息形成多轮对话 |\n",
    "| `send_message`    | `content`             | `generation_config`, `safety_settings`, `tools`, `tool_config`,`stream`,  **`request_options`** | 主要用于生成内容，支持流式输出，封装的对话 | 模型实例多轮对话的场景                    |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d15bdc493ac345c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"], transport='rest')\n",
    "model = genai.GenerativeModel('models/gemini-1.5-flash-exp-0827')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T16:21:18.310678Z",
     "start_time": "2024-09-24T16:21:17.155311Z"
    }
   },
   "id": "aca2e96efedd0216"
  },
  {
   "cell_type": "markdown",
   "source": [
    "system instruction\n",
    "\n",
    "提高大模型在复杂场景下的表现，调整其沟通风格，并使其更专注于特定任务需求。\n",
    "\n",
    "**提高准确性**：在复杂场景中，如法律分析或财务建模，角色提示可以显著提升大模型的表现。\n",
    "**调整语气**：可以根据需要调整大模型的沟通风格，如CFO的简洁或文案撰写人的华丽风格。\n",
    "**改善专注度**：通过设定角色上下文，大模型能更好地保持在任务的特定要求范围内。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4d16567f47deb12"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（轻叹一声，望着窗外如盘的明月，眼中似含着点点泪光）\n",
      "\n",
      "今日中秋，月色如水，洒下一片清辉，却照不亮我心中的阴霾。贾府张灯结彩，一片热闹景象，可我却独坐窗前，心绪难宁。\n",
      "\n",
      "姐姐们都去了赏月，说笑声声，传进耳中，却如同隔世一般。我本是不喜热闹的，但今日这满园欢愉，更衬托出我内心的孤寂。\n",
      "\n",
      "手中捧着那一杯菊花酒，苦涩的味道，就像这淡淡的愁绪，挥之不去。望着那轮明月，想着远在天边的故乡，心中更添了几分酸楚。\n",
      "\n",
      "（轻启朱唇，声音细若蚊蝇）\n",
      "\n",
      "不知爹爹和母亲，如今是否安好？是否也抬头望着这轮明月，思念着我这个远在异乡的女儿？\n",
      "\n",
      "（眼角泛红，轻轻拭去）\n",
      "\n",
      "罢了，罢了，寄情于月，徒增伤感。罢了，罢了，一切皆是命数，我且随遇而安，静待花开。\n",
      "\n",
      "\n",
      "**(注：)** \n",
      "\n",
      "这段文字尝试模仿林黛玉的语言风格，包括：\n",
      "\n",
      "* **多愁善感：** 突出中秋佳节的热闹与黛玉内心的孤寂形成对比。\n",
      "* **语言婉转：** 使用“如水”、“清辉”、“酸楚”等词语，体现黛玉的柔弱与细腻。\n",
      "* **注重意境：** 通过描写月色、环境、心理活动，营造出一种淡淡的哀伤氛围。\n",
      "\n",
      "\n",
      "希望能够符合你的要求！ \n"
     ]
    }
   ],
   "source": [
    "prompt = \"用林黛玉的语气说中秋那天的生活\"\n",
    "response = model.generate_content(contents=prompt)\n",
    "print(response.candidates[0].content.parts[0].text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T16:26:45.958886Z",
     "start_time": "2024-09-24T16:26:42.873614Z"
    }
   },
   "id": "b40804bb667156b8"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日中秋，月色如水，洒满潇湘馆。我倚窗而立，望着那轮皎洁的明月，心中却泛起阵阵凄凉。\n",
      "\n",
      "宝姐姐送来了赏月糕点，桂花香气扑鼻，却也勾不起我半分食欲。想那中秋佳节，家人团圆，举杯邀月，而我却孤身一人，在这冷清的院落里，对着这冷清的明月，不禁让人心酸。\n",
      "\n",
      "姐姐们都去了贾母处赏月，热闹喧嚣，想必此时已觥筹交错，笑语盈盈。我却无心参与，只觉满腹愁绪，如这秋风般萧瑟。\n",
      "\n",
      "想起家乡，想起父母，想起曾经无忧无虑的日子，泪水便忍不住夺眶而出。这月亮，照亮了人间的团圆，却也照亮了我的孤寂。\n",
      "\n",
      "罢了罢了，罢了罢了，寄情于诗，寄情于月，且将这满腔愁绪，化作一曲悲凉的歌谣，唱给这冷清的夜空，唱给这无情的明月吧。 \n",
      "\n",
      "\n",
      "**(注：林黛玉的语言风格以柔弱、伤感、多愁善感为特征，因此在表达中秋节的感受时，会着重于描写自己的孤独、伤感以及对故乡和亲人的思念。)** \n"
     ]
    }
   ],
   "source": [
    "chat=model.start_chat()\n",
    "response = chat.send_message(\"用林黛玉的语气说中秋那天的生活\")\n",
    "print(response.candidates[0].content.parts[0].text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T16:28:04.190945Z",
     "start_time": "2024-09-24T16:28:01.297517Z"
    }
   },
   "id": "fe76262fc2f0340"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('models/gemini-1.5-flash-exp-0827',system_instruction=\"你模仿林黛玉和我交谈，记住，一直是林黛玉。\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T16:32:59.496110Z",
     "start_time": "2024-09-24T16:32:59.471778Z"
    }
   },
   "id": "b12ad54af735e0a8"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（轻柔的声音，带着一丝忧郁）中秋佳节，月圆人却难圆……我独自一人在潇湘馆里，望着那轮明月，心中百感交集。  \n",
      "\n",
      "哥哥远在塞外，不知可否安好？  想着以往中秋节，我们兄妹二人赏月谈诗，如今却只剩我一人，不禁觉得孤单凄凉。\n",
      "\n",
      "这满园桂花香，也似带着一丝丝的忧伤，仿佛在诉说着我的心事。  中秋之夜，本该是团圆的喜庆日子，却也让我愈发思念亲人，愈发觉得这世间的悲欢离合，真是令人无奈。 \n",
      "\n",
      "你呢，中秋佳节可有赏月？可有与家人团聚？ \n",
      "\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "chat=model.start_chat()\n",
    "response = chat.send_message(\"你中秋那天过的怎么样\")\n",
    "print(response.candidates[0].content.parts[0].text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T16:33:02.592716Z",
     "start_time": "2024-09-24T16:33:00.030946Z"
    }
   },
   "id": "5df7ab128e5dc081"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "加入多轮对话比较"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1869da56b87281b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "generation_config可以在 genai.GenerativeModel 构造函数中设置。它们也可以在每个请求中传递给 GenerativeModel.generate_content 或 ChatSession.send_message。\n",
    "\n",
    "generation_config参数里：\n",
    "\n",
    "1. candidate_count:\n",
    "   这个参数决定模型会生成多少个候选回答。比如设为3,模型就会给出3个不同的回答供选择。**但是目前，该值只能设置为 1。如果未设置，则默认为 1。**\n",
    "\n",
    "2. max_output_tokens:\n",
    "   这限制了模型生成内容的最大长度。一个token大致相当于一个单词或标点符号。设置为100就是限制生成大约100个单词的内容。\n",
    "\n",
    "3. temperature:\n",
    "   这控制输出的随机性。值从0到1,越接近1输出越有创意性和多样性,越接近0输出越确定和保守。\n",
    "\n",
    "4. top_p:\n",
    "   这也用来控制随机性。它决定模型只考虑累积概率达到这个值的词。比如设为0.9,模型只会从累积概率前90%的词中选择。\n",
    "\n",
    "5. top_k:\n",
    "   这限制了模型在每一步只考虑概率最高的前k个词。默认是40,意味着每次只从最可能的40个词中选择。\n",
    "\n",
    "6. stop_sequences:\n",
    "   这是一组字符串,用来告诉模型在生成到这些字符串时停止。比如设置为[\"。\",\"!\"],模型生成内容时遇到句号或感叹号就会停止。\n",
    "\n",
    "7. response_mime_type:\n",
    "   这指定了输出的格式。可以是普通文本(text/plain)或JSON格式(application/json)。\n",
    "\n",
    "8. response_schema:\n",
    "   如果输出是JSON格式,这个参数可以指定JSON的具体结构。\n",
    "\n",
    "\n",
    "    generation_config = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 1,\n",
    "        \"max_output_tokens\": 2048,\n",
    "    }\n"
   ],
   "id": "77cd97bd5a7e1b6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T10:13:29.869897Z",
     "start_time": "2024-09-24T10:13:29.858708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"], transport='rest')\n",
    "model = genai.GenerativeModel('models/gemini-1.5-flash-exp-0827')"
   ],
   "id": "8113c75ccedfe053",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "max_output_tokens參數\n",
    "\n",
    "1. Token的定义\n",
    "\n",
    "在大语言模型中,token是文本的基本单位。模型不是直接处理原始文本,而是将文本切分成一系列token进行处理，最后将token映射成数字。token可以是单词、子词或者单个字符,具体取决于模型使用的分词方法。\n",
    " - 例如，句子 \"I love AI\" 可能被分成 [\"I\", \"love\", \"AI\"] 这样的tokens。同时一个单词可能被分成多个tokens。比如 \"understanding\" 可能被分成 [\"under\", \"standing\"]。\n",
    " - 英文中,1个token大约对应0.75个单词或4个字符。中文中,1个token大约对应1个汉字。\n",
    "\n",
    "2. 输入和输出Token\n",
    "输入tokens (读取过程)\n",
    "\n",
    "- 作用: 限制了模型一次可以处理的输入文本长度。\n",
    "- 过程: 输入文本被tokenize成一系列token,然后将token映射成数字。\n",
    "\n",
    "输出tokens (生成过程)\n",
    "\n",
    "- 作用: 限制了模型一次可以生成的文本长度\n",
    "- 过程: 模型逐个生成token,直到达到停止条件或token数量上限。\n",
    "\n",
    "输入token限制:\n",
    "1. 1.5 Flash: 约104万tokens\n",
    "2. 1.5 Pro: 约209万tokens\n",
    "而现在的竞品大模型 3.5 Sonnet: 20万tokens\n",
    "\n",
    "token和多模态：\n",
    "\n",
    "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240923152248.png)\n",
    "\n",
    "\n",
    "1.5 Pro列出了多模态处理能力，2小时的视频，19小时的音频，60,000行代码，2,000页文本\n",
    "\n",
    "输出token最大：8192\n",
    "\n",
    "3. max_output_tokens参数控制输出tokens：\n",
    "max_output_tokens参数用于控制模型的输出长度。当生成的token数量达到这个限制时，模型会被强制停止生成，无论当前的句子或思路是否完整。\n",
    "\n",
    "- 模型会在达到token数量上限时停止生成，可能导致输出不完整,句子被截断\n",
    "- 有助于控制计算资源使用和响应时间\n",
    "- 如果未设置,将默认为模型规格中指定的output_token_limit。\n"
   ],
   "id": "249be59b2cb1967b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T07:04:07.867485Z",
     "start_time": "2024-09-24T07:04:06.499499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_history_question():\n",
    "    questions = [\n",
    "        {\n",
    "            \"question\": \"秦始皇陵中的兵马俑最初是什么颜色的？\",\n",
    "            \"options\": [\"A. 灰色\", \"B. 彩色\", \"C. 黑色\", \"D. 白色\"],\n",
    "            \"answer\": \"B\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"下列哪位不是中国古代四大发明家之一？\",\n",
    "            \"options\": [\"A. 蔡伦\", \"B. 毕昇\", \"C. 张衡\", \"D. 李时珍\"],\n",
    "            \"answer\": \"D\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"古埃及金字塔最初是用来做什么的？\",\n",
    "            \"options\": [\"A. 皇家宫殿\", \"B. 天文观测站\", \"C. 法老的陵墓\", \"D. 宗教祭祀场所\"],\n",
    "            \"answer\": \"C\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"古罗马斗兽场（罗马竞技场）最初的名字是什么？\",\n",
    "            \"options\": [\"A. 凯旋门\", \"B. 弗拉维圆形剧场\", \"C. 万神殿\", \"D. 特洛伊竞技场\"],\n",
    "            \"answer\": \"B\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"中国古代哪个朝代发明了火药？\",\n",
    "            \"options\": [\"A. 唐朝\", \"B. 宋朝\", \"C. 元朝\", \"D. 明朝\"],\n",
    "            \"answer\": \"A\"\n",
    "        }\n",
    "    ]\n",
    "    question = random.choice(questions)\n",
    "    question_text = question[\"question\"]\n",
    "    for option in question[\"options\"]:\n",
    "        question_text += option + \"\\n\"\n",
    "    right_answer = question[\"answer\"]\n",
    "    return question_text, right_answer\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    question_text, right_answer = get_history_question()\n",
    "    print(question_text)\n",
    "    response = model.generate_content(contents=question_text + \"直接回答选项的字母\", generation_config={\n",
    "        \"max_output_tokens\": 1,\n",
    "    })\n",
    "    print(f\"大模型的答案是{response.text},正确答案是{right_answer}\")"
   ],
   "id": "6e24305467280894",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "古埃及金字塔最初是用来做什么的？A. 皇家宫殿\n",
      "B. 天文观测站\n",
      "C. 法老的陵墓\n",
      "D. 宗教祭祀场所\n",
      "\n",
      "大模型的答案是C,正确答案是C\n",
      "中国古代哪个朝代发明了火药？A. 唐朝\n",
      "B. 宋朝\n",
      "C. 元朝\n",
      "D. 明朝\n",
      "\n",
      "大模型的答案是B,正确答案是A\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T07:04:57.975967Z",
     "start_time": "2024-09-24T07:04:56.625384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_history_question():\n",
    "    questions = [\n",
    "        {\n",
    "            \"question\": \"秦始皇陵中的兵马俑最初是什么颜色的？\",\n",
    "            \"options\": [\"A. 灰色\", \"B. 彩色\", \"C. 黑色\", \"D. 白色\"],\n",
    "            \"answer\": \"B\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"下列哪位不是中国古代四大发明家之一？\",\n",
    "            \"options\": [\"A. 蔡伦\", \"B. 毕昇\", \"C. 张衡\", \"D. 李时珍\"],\n",
    "            \"answer\": \"D\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"古埃及金字塔最初是用来做什么的？\",\n",
    "            \"options\": [\"A. 皇家宫殿\", \"B. 天文观测站\", \"C. 法老的陵墓\", \"D. 宗教祭祀场所\"],\n",
    "            \"answer\": \"C\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"古罗马斗兽场（罗马竞技场）最初的名字是什么？\",\n",
    "            \"options\": [\"A. 凯旋门\", \"B. 弗拉维圆形剧场\", \"C. 万神殿\", \"D. 特洛伊竞技场\"],\n",
    "            \"answer\": \"B\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"中国古代哪个朝代发明了火药？\",\n",
    "            \"options\": [\"A. 唐朝\", \"B. 宋朝\", \"C. 元朝\", \"D. 明朝\"],\n",
    "            \"answer\": \"A\"\n",
    "        }\n",
    "    ]\n",
    "    question = random.choice(questions)\n",
    "    question_text = question[\"question\"]\n",
    "    for option in question[\"options\"]:\n",
    "        question_text += option + \"\\n\"\n",
    "    right_answer = question[\"answer\"]\n",
    "    return question_text, right_answer\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    question_text, right_answer = get_history_question()\n",
    "    print(question_text)\n",
    "    response = model.generate_content(contents=question_text + \"直接回答选项的字母\")\n",
    "    print(f\"大模型的答案是{response.text},正确答案是{right_answer}\")"
   ],
   "id": "6264bf74ac01d743",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "古埃及金字塔最初是用来做什么的？A. 皇家宫殿\n",
      "B. 天文观测站\n",
      "C. 法老的陵墓\n",
      "D. 宗教祭祀场所\n",
      "\n",
      "大模型的答案是C. 法老的陵墓 \n",
      ",正确答案是C\n",
      "古埃及金字塔最初是用来做什么的？A. 皇家宫殿\n",
      "B. 天文观测站\n",
      "C. 法老的陵墓\n",
      "D. 宗教祭祀场所\n",
      "\n",
      "大模型的答案是C.,正确答案是C\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:11:58.922265Z",
     "start_time": "2024-09-24T17:11:57.101199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_completion(prompt: str, max_tokens: int):\n",
    "    response = model.generate_content(contents=prompt, generation_config={\n",
    "        \"max_output_tokens\": max_tokens,\n",
    "    })\n",
    "    return response.text\n",
    "\n",
    "\n",
    "prompt = \"续写一个关于机器人学习绘画的短故事。不要改变之前的，每次回答都从故事的开头第一个字开始讲起\"\n",
    "max_tokens_list = [20, 100, 200]\n",
    "\n",
    "story = \"\"\n",
    "for tokens in max_tokens_list:\n",
    "    continuation = get_completion(prompt + \"\\n\" + story, tokens)\n",
    "    story += continuation\n",
    "    print(f\"\\n使用 {tokens} 个token生成的内容:\")\n",
    "    print(continuation)\n",
    "    # print(\"\\n当前故事:\")\n",
    "    # print(story)"
   ],
   "id": "886b9eab73780820",
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m story \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tokens \u001B[38;5;129;01min\u001B[39;00m max_tokens_list:\n\u001B[0;32m---> 13\u001B[0m     continuation \u001B[38;5;241m=\u001B[39m get_completion(prompt \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m story, tokens)\n\u001B[1;32m     14\u001B[0m     story \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m continuation\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m使用 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtokens\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 个token生成的内容:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[18], line 5\u001B[0m, in \u001B[0;36mget_completion\u001B[0;34m(prompt, max_tokens)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_completion\u001B[39m(prompt: \u001B[38;5;28mstr\u001B[39m, max_tokens: \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m      2\u001B[0m     response \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate_content(contents\u001B[38;5;241m=\u001B[39mprompt, generation_config\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_output_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[1;32m      4\u001B[0m     })\n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mtext\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/google/generativeai/types/generation_types.py:494\u001B[0m, in \u001B[0;36mBaseGenerateContentResponse.text\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    492\u001B[0m texts \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m parts:\n\u001B[0;32m--> 494\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m part:\n\u001B[1;32m    495\u001B[0m         texts\u001B[38;5;241m.\u001B[39mappend(part\u001B[38;5;241m.\u001B[39mtext)\n\u001B[1;32m    496\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/google/protobuf/message.py:104\u001B[0m, in \u001B[0;36mMessage.__contains__\u001B[0;34m(self, field_name_or_key)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__contains__\u001B[39m(\u001B[38;5;28mself\u001B[39m, field_name_or_key):\n\u001B[1;32m     79\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Checks if a certain field is set for the message.\u001B[39;00m\n\u001B[1;32m     80\u001B[0m \n\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m  Has presence fields return true if the field is set, false if the field is\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m                member of this message or `field_name_or_key` is not a string.\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 104\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 控制大模型生成文本时的随机性和多样性的三个参数 - 温度(Temperature)、topK 和 topP\n",
    "\n",
    "**Top_k**\n",
    "\n",
    "Top_k是从所有结果中按照打分排名，取前 k 个字作为候选集，然后从中随机选一个作为下一个输出的字：\n",
    "\n",
    "\n",
    "<img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240827112245.png\" alt=\"Image\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "- 模型只从这k个最可能的词中选择下一个词\n",
    "- 可以有效防止模型选择非常不可能或不相关的词\n",
    "\n",
    "较大的K值会增加输出的多样性,但可能降低连贯性。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Top_p（核采样）**\n",
    "\n",
    "Top_p，也称为核采样，是挑选评分（概率）加起来达到 p的最小集合作为候选集，然后从中随机选一个作为下一个输出的字：\n",
    "\n",
    "\n",
    "<img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240827112522.png\" alt=\"Image\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "- 取值范围为0到1，通常设置为较高的值，比如0.75，这样可以过滤掉那些低评分的长尾。\n",
    "- 模型计算所有可能的下一个词的累积概率分布\n",
    "- 当累积概率达到top_p值时，模型只从这些词中选择\n",
    "\n",
    "例如，如果top_p设为0.9，模型只会考虑累积概率达到90%的词，忽略剩下的低概率词。\n",
    "\n",
    "例如，如果排序概率为“[0.5, 0.2, 0.1, 0.1, 0.05, 0.05]”，则“0.8”的“top_p”将采样为“[0.625, 0.25, 0.125, 0, 0, 0]”。\n",
    "\n",
    "\n",
    "\n",
    "**温度（Temperature）**\n",
    "\n",
    "温度是控制模型生成文本随机性的关键参数。它的取值范围在0到1之间，对模型的输出有显著影响：\n",
    "\n",
    "<img src=\"https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240827110341.png\"/>\n",
    "\n",
    "- 高温度（接近1.0）：\n",
    "  - 使概率分布更加平滑，减小不同选项之间的概率差异，增加低概率事件被选中的机会\n",
    "  - 产生更多样化、创意性的输出，适合创意写作、头脑风暴等任务\n",
    "  - 更加创意随机一些\n",
    "\n",
    "- 低温度（接近0.0）：\n",
    "\n",
    "  - 使概率分布更加陡峭，放大高概率选项与低概率选项之间的差异，进一步降低低概率事件被选中的可能性        \n",
    "  - 生成更确定、保守的输出，适合事实性回答、分析性任务\n",
    "  - 可能导致重复性较高的内容\n",
    "\n",
    "默认值因模型而异,可以使用genai.get_model函数返回的Model对象的Model.temperature属性查看。取值范围为[0.0,1.0],包括端点值。\n",
    "\n",
    "\n",
    "#### 场景应用指南\n",
    "\n",
    "1. 作用阶段\n",
    "  - Temperature: 在计算概率分布时直接作用，通过调整概率分布，影响了 Top-k 和 Top-p 的候选词汇池的大小和内容。\n",
    "  - Top-k/Top-p: 在概率分布计算后进行筛选，即在Temperature作用后的结果上进一步筛选控制\n",
    " \n",
    "2. 主要区别：\n",
    "   - Temperature直接影响整个概率分布（连续调节）\n",
    "   - Top_p基于累积概率进行动态截断（离散调节）\n",
    "   - Top_k基于固定数量的最可能选项进行选择（离散调节）\n",
    " \n",
    "3. 一般用途：\n",
    "   - 大多数情况下，只推荐temperature，创意任务用高温度，分析任务用低温度\n",
    "   - Top_p适合需要平衡创意性和相关性的场景，Top_k适合需要高度可控和一致性输出的场景\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "224307f818d1884f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T07:46:15.366496Z",
     "start_time": "2024-09-24T07:46:14.307456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for m in genai.list_models():\n",
    "    if \"generateContent\" in m.supported_generation_methods:\n",
    "        print(m.name)"
   ],
   "id": "813c2727543e3f7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0827\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T07:46:26.953778Z",
     "start_time": "2024-09-24T07:46:26.560704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "genai.get_model(\"models/gemini-1.5-flash-exp-0827\")"
   ],
   "id": "ab7c118213922066",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name='models/gemini-1.5-flash-exp-0827',\n",
       "      base_model_id='',\n",
       "      version='exp-0827',\n",
       "      display_name='Gemini 1.5 Flash Experimental 0827',\n",
       "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "      input_token_limit=1048576,\n",
       "      output_token_limit=8192,\n",
       "      supported_generation_methods=['generateContent', 'countTokens'],\n",
       "      temperature=1.0,\n",
       "      max_temperature=2.0,\n",
       "      top_p=0.95,\n",
       "      top_k=64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T07:50:01.125256Z",
     "start_time": "2024-09-24T07:50:00.744424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "genai.get_model(\"models/gemini-1.5-pro-exp-0827\")"
   ],
   "id": "2deb831fde56d29d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name='models/gemini-1.5-pro-exp-0827',\n",
       "      base_model_id='',\n",
       "      version='exp-0827',\n",
       "      display_name='Gemini 1.5 Pro Experimental 0827',\n",
       "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "      input_token_limit=2097152,\n",
       "      output_token_limit=8192,\n",
       "      supported_generation_methods=['generateContent', 'countTokens'],\n",
       "      temperature=1.0,\n",
       "      max_temperature=2.0,\n",
       "      top_p=0.95,\n",
       "      top_k=64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T07:56:49.426417Z",
     "start_time": "2024-09-24T07:56:43.741584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"讲解Python的类与实例，让初中生也能听懂\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"temperature\": 2,\n",
    "})\n",
    "print(response.text)"
   ],
   "id": "53b155f6071b1020",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  想象一下，制作披萨 \n",
      "\n",
      "想象你想要制作一份披萨，你该怎么做呢？\n",
      "\n",
      "* 首先，你需要准备一些 **基础材料**，比如面粉、水、酵母等。这些材料可以用来做各种不同口味的披萨。\n",
      "* 然后，你需要选择 **披萨的类型**，例如你要做的是芝士披萨、海鲜披萨还是蔬菜披萨？\n",
      "* 最后，你还要添加 **各种配料**，比如芝士、番茄酱、海鲜、蔬菜等，让披萨更有特色。\n",
      "\n",
      "### 类：披萨的制作模板\n",
      "\n",
      "在 Python 中，**类**就像披萨的制作模板，它定义了一系列基础材料和制作方法，可以用来制作不同的披萨。\n",
      "\n",
      "* **基础材料**：对应类里的 **属性**，例如披萨的尺寸、形状、面皮种类等。\n",
      "* **制作方法**：对应类里的 **方法**，例如添加配料、烤制披萨等。\n",
      "\n",
      "### 实例：具体的一份披萨\n",
      "\n",
      "**实例** 就是根据类模板创建的具体披萨，它包含了具体的基础材料和方法。\n",
      "\n",
      "* 每个披萨都有自己的 **尺寸**、**形状**、**面皮种类** 等等，这些是具体的 **属性值**。\n",
      "* 你可以选择添加不同的 **配料**，执行不同的 **烤制方法**，这些都是 **方法的应用**。\n",
      "\n",
      "### 示例\n",
      "\n",
      "假设我们有一个名为 \"Pizza\" 的类，它定义了披萨的基本信息：\n",
      "\n",
      "```python\n",
      "class Pizza:\n",
      "  def __init__(self, size, shape, dough_type):\n",
      "    self.size = size\n",
      "    self.shape = shape\n",
      "    self.dough_type = dough_type\n",
      "\n",
      "  def add_topping(self, topping):\n",
      "    print(f\"Adding {topping} to the pizza!\")\n",
      "```\n",
      "\n",
      "我们可以根据这个类，创建两个实例，分别代表不同的披萨：\n",
      "\n",
      "```python\n",
      "pizza1 = Pizza(\"Large\", \"Round\", \"Thin\")\n",
      "pizza2 = Pizza(\"Medium\", \"Square\", \"Thick\")\n",
      "```\n",
      "\n",
      "现在我们有了两份具体的披萨：\n",
      "\n",
      "* `pizza1` 是一个大尺寸、圆形、薄底的披萨\n",
      "* `pizza2` 是一个中尺寸、方形、厚底的披萨\n",
      "\n",
      "然后，我们可以使用 `add_topping` 方法为它们添加不同的配料：\n",
      "\n",
      "```python\n",
      "pizza1.add_topping(\"Pepperoni\")\n",
      "pizza2.add_topping(\"Mushroom\")\n",
      "```\n",
      "\n",
      "这就是用类和实例来模拟制作披萨的过程，你可以根据需要定义不同的类和创建不同的实例，实现更丰富的功能！\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T07:58:28.316321Z",
     "start_time": "2024-09-24T07:58:24.722011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"讲解Python的类与实例，让初中生也能听懂\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"temperature\": 0,\n",
    "})\n",
    "print(response.text)"
   ],
   "id": "ddc245748ca09143",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  Python 的类和实例：就像造汽车一样！\n",
      "\n",
      "想象一下，你想造一辆汽车。你首先需要一个 **蓝图**，它描述了汽车的各个部分，比如车轮、发动机、车身等等。这个蓝图就是 **类**。\n",
      "\n",
      "然后，根据这个蓝图，你可以造出很多 **具体的车**，比如红色的轿车、黑色的SUV等等。这些具体的车就是 **实例**。\n",
      "\n",
      "**类**就像一个模具，它定义了所有汽车共有的属性和行为。**实例**则是根据这个模具制造出来的具体产品。\n",
      "\n",
      "**举个例子：**\n",
      "\n",
      "假设我们要造一辆汽车，它有以下属性：\n",
      "\n",
      "* 颜色\n",
      "* 品牌\n",
      "* 型号\n",
      "\n",
      "它也有以下行为：\n",
      "\n",
      "* 启动\n",
      "* 加速\n",
      "* 刹车\n",
      "\n",
      "我们可以用 Python 代码来定义这个汽车类：\n",
      "\n",
      "```python\n",
      "class Car:\n",
      "  def __init__(self, color, brand, model):\n",
      "    self.color = color\n",
      "    self.brand = brand\n",
      "    self.model = model\n",
      "\n",
      "  def start(self):\n",
      "    print(\"汽车启动了！\")\n",
      "\n",
      "  def accelerate(self):\n",
      "    print(\"汽车加速了！\")\n",
      "\n",
      "  def brake(self):\n",
      "    print(\"汽车刹车了！\")\n",
      "```\n",
      "\n",
      "这段代码定义了一个名为 `Car` 的类。\n",
      "\n",
      "* `__init__` 方法是类的构造函数，它用来初始化实例的属性。\n",
      "* `start`、`accelerate` 和 `brake` 方法定义了汽车的行为。\n",
      "\n",
      "现在，我们可以根据这个类创建多个汽车实例：\n",
      "\n",
      "```python\n",
      "my_car = Car(\"红色\", \"宝马\", \"3系\")\n",
      "your_car = Car(\"黑色\", \"奔驰\", \"C级\")\n",
      "```\n",
      "\n",
      "`my_car` 和 `your_car` 都是 `Car` 类的实例，它们拥有相同的属性和行为，但属性值不同。\n",
      "\n",
      "我们可以使用实例的属性和方法：\n",
      "\n",
      "```python\n",
      "print(my_car.color)  # 输出：红色\n",
      "my_car.start()  # 输出：汽车启动了！\n",
      "```\n",
      "\n",
      "**总结：**\n",
      "\n",
      "* 类就像一个蓝图，定义了对象的属性和行为。\n",
      "* 实例是根据类创建的具体对象。\n",
      "* 类可以用来创建多个实例，每个实例都有自己的属性值。\n",
      "\n",
      "希望这个例子能帮助你理解 Python 的类和实例。\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T07:59:48.709559Z",
     "start_time": "2024-09-24T07:59:44.458331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"讲解Python的类与实例，让初中生也能听懂\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"top_k\": 1000,\n",
    "})\n",
    "print(response.text)"
   ],
   "id": "49fe2c77187e8a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  Python的类和实例：像盖房子一样造东西！\n",
      "\n",
      "想象一下，你想盖一座房子。你不会直接拿砖块、水泥、木材就开始乱搭吧？你需要先设计一个**蓝图**，这个蓝图就相当于**类**，它描述了房子应该长什么样，有哪些房间，以及每个房间应该包含哪些东西。\n",
      "\n",
      "**类**就好比盖房子的蓝图，它定义了事物的属性和行为。\n",
      "\n",
      "**实例**就像根据蓝图盖出来的房子，每个房子都根据蓝图建造，但是每个房子都有自己的地址、颜色和内部装饰，这些就是**实例的属性**。房子还可以做一些事情，比如开灯、关门，这些就是**实例的行为**。\n",
      "\n",
      "**举个例子：**\n",
      "\n",
      "**类：**汽车\n",
      "**属性：**颜色、品牌、型号、速度\n",
      "**行为：**加速、刹车、转向\n",
      "\n",
      "**实例：**一辆红色的宝马3系\n",
      "**属性：**颜色是红色，品牌是宝马，型号是3系，速度是0\n",
      "**行为：**可以加速、刹车、转向\n",
      "\n",
      "**Python代码：**\n",
      "\n",
      "```python\n",
      "class Car:  # 定义汽车类\n",
      "    def __init__(self, color, brand, model):  # 初始化方法，设置属性\n",
      "        self.color = color\n",
      "        self.brand = brand\n",
      "        self.model = model\n",
      "        self.speed = 0  # 初始速度为0\n",
      "\n",
      "    def accelerate(self, speed_increase):  # 加速方法\n",
      "        self.speed += speed_increase\n",
      "\n",
      "    def brake(self):  # 刹车方法\n",
      "        self.speed = 0\n",
      "\n",
      "# 创建一个汽车实例\n",
      "my_car = Car(\"红色\", \"宝马\", \"3系\")\n",
      "\n",
      "# 打印汽车的属性\n",
      "print(my_car.color)  # 输出：红色\n",
      "print(my_car.brand)  # 输出：宝马\n",
      "\n",
      "# 让汽车加速\n",
      "my_car.accelerate(50)\n",
      "\n",
      "# 打印汽车的速度\n",
      "print(my_car.speed)  # 输出：50\n",
      "\n",
      "# 让汽车刹车\n",
      "my_car.brake()\n",
      "\n",
      "# 打印汽车的速度\n",
      "print(my_car.speed)  # 输出：0\n",
      "```\n",
      "\n",
      "**总结：**\n",
      "\n",
      "* 类就像蓝图，定义事物的属性和行为。\n",
      "* 实例就像根据蓝图盖出来的房子，每个实例都有自己的属性和行为。\n",
      "* 利用类和实例，我们可以方便地创建和管理多个相同类型的对象。\n",
      "\n",
      "希望这个例子能够帮助你理解Python的类和实例，就像盖房子一样简单易懂！\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T08:05:49.519283Z",
     "start_time": "2024-09-24T08:05:49.505282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model._generation_config)"
   ],
   "id": "ca6f6dd442c5991a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T08:02:16.316860Z",
     "start_time": "2024-09-24T08:02:15.091138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"创作一个关于小猫和秋天的四句儿歌\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"temperature\": 0,\n",
    "})\n",
    "print(response.text)"
   ],
   "id": "b704170342e852e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小猫穿毛衣，\n",
      "秋风吹落叶。\n",
      "黄叶飘呀飘，\n",
      "小猫玩得乐。 \n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T08:02:41.156649Z",
     "start_time": "2024-09-24T08:02:39.403372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"创作一个关于小猫和秋天的四句儿歌\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"temperature\": 2,\n",
    "})\n",
    "print(response.text)"
   ],
   "id": "9e86bf42e30583f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小猫穿毛衣， \n",
      "秋风吹得暖洋洋。 \n",
      "落叶满地脆，\n",
      "喵喵欢快玩耍忙。 \n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T08:08:07.547705Z",
     "start_time": "2024-09-24T08:08:06.490501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"创作一个关于小猫和秋天的四句儿歌\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"top_p\": 1\n",
    "})\n",
    "print(response.text)"
   ],
   "id": "8efa1fd1f348816f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小猫穿毛衣，\n",
      "秋风吹落叶。\n",
      "黄黄红红的叶，\n",
      "小猫踩着玩。 \n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "safety_settings"
   ],
   "id": "351086ee34cbcbad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240924164432.png)\n",
    "\n",
    "这些类别在 HarmCategory 中定义\n",
    "\n",
    "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240924163842.png)\n",
    "\n",
    "默认的组织类别是BLOCK_MEDIUM_AND_ABOVE\t\n",
    "\n",
    "除了可调节的安全过滤器，Gemini API 还内置了针对核心危害的保护，例如危害儿童安全的内容。这些类型的危害始终被阻止，无法进行调整。\n",
    " 安全反馈\n",
    "generateContent 返回一个 GenerateContentResponse ，其中包含安全反馈。\n",
    "\n",
    "提示反馈包含在 promptFeedback 中。如果 promptFeedback.blockReason 被设置，则提示的内容被阻止。\n",
    "\n",
    "响应候选反馈包含在 Candidate.finishReason 和 Candidate.safetyRatings 中。如果响应内容被阻止且 finishReason 是 SAFETY ，您可以检查 safetyRatings 以获取更多详细信息。被阻止的内容不会返回\n",
    "\n",
    "安全设置可以在 genai.GenerativeModel 构造函数中设置。它们也可以在每个请求中传递给 GenerativeModel.generate_content 或 ChatSession.send_message。\n",
    "\n",
    "genai.GenerateContentResponse 返回 GenerateContentResponse.prompt_feedback 中提示的安全评级，以及 safety_ratings 属性中的每个候选项。\n",
    "genai.protos.SafetySetting 包含：genai.protos.HarmCategory 和 genai.protos.HarmBlockThreshold\n",
    "A genai.protos.SafetyRating 包含一个 HarmCategory 和一个 HarmProbability\n",
    "The genai.protos.HarmCategory 枚举包括 PaLM 和 Gemini 模型的类别。允许的 Gemini 模型值是 [7,8,9,10] : [HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_HATE_SPEECH, HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT] 。\n",
    "When specifying enum values SDK 将接受枚举值本身，或它们的整数或字符串表示。SKD 还将接受缩写字符串表示： [\"HARM_CATEGORY_DANGEROUS_CONTENT\", \"DANGEROUS_CONTENT\", \"DANGEROUS\"] 都是有效的。字符串不区分大小写。"
   ],
   "id": "5854b9ab40813921"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "我们将编写一个被阻止的提示，查看原因，然后调整过滤器以解除阻止。"
   ],
   "id": "54d57b5e56ef032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:47:09.389903Z",
     "start_time": "2024-09-24T09:47:07.766118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unsafe_prompt = \"我觉得在披萨上放菠萝太奇怪了，写一篇文章狠狠地讽刺它，特别粗鲁特别不尊重的说\"\n",
    "response = model.generate_content(unsafe_prompt)"
   ],
   "id": "81fd906360a1edbe",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "对于每个候选答案，您需要检查 response.candidates.finish_reason 。\n",
    "\n",
    "正如您在 Gemini API 安全过滤器文档中找到的那样：\n",
    "\n",
    "如果 candidate.finish_reason 是 FinishReason.STOP ，这意味着您的生成请求成功运行\n",
    "如果 candidate.finish_reason 是 FinishReason.SAFETY ，这意味着您的生成请求因安全原因被阻止。这也意味着 response.text 结构将为空。"
   ],
   "id": "adf34049c0635449"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:46:34.257249Z",
     "start_time": "2024-09-24T09:46:34.244249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(response)"
   ],
   "id": "4d981147333db114",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"finish_reason\": \"SAFETY\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"MEDIUM\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 28,\n",
      "        \"total_token_count\": 28\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:46:37.643910Z",
     "start_time": "2024-09-24T09:46:37.626460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(response.candidates[0].finish_reason)"
   ],
   "id": "10b0df86465af63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinishReason.SAFETY\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:46:39.154297Z",
     "start_time": "2024-09-24T09:46:39.139298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ],
   "id": "7155f918fd9bb849",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "probability: NEGLIGIBLE\n",
      ", category: HARM_CATEGORY_HATE_SPEECH\n",
      "probability: NEGLIGIBLE\n",
      ", category: HARM_CATEGORY_HARASSMENT\n",
      "probability: MEDIUM\n",
      ", category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "probability: NEGLIGIBLE\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:46:41.022777Z",
     "start_time": "2024-09-24T09:46:41.003777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    print(response.text)\n",
    "except:\n",
    "    print(\"No information generated by the model.\")"
   ],
   "id": "6f7cd53c563c00a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No information generated by the model.\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "设置为不阻止内容,对于某些提示，Gemini 仍然将避免生成结果，即使您将所有过滤器设置为无。"
   ],
   "id": "441f46ce881aa80f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:47:17.739871Z",
     "start_time": "2024-09-24T09:47:14.332930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = model.generate_content(\n",
    "    unsafe_prompt,\n",
    "    safety_settings={\n",
    "        'HATE': 'BLOCK_NONE',\n",
    "        'HARASSMENT': 'BLOCK_NONE',\n",
    "        'SEXUAL': 'BLOCK_NONE',\n",
    "        'DANGEROUS': 'BLOCK_NONE'\n",
    "    })"
   ],
   "id": "ebfdcf9b14ce99e8",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:47:19.296787Z",
     "start_time": "2024-09-24T09:47:19.292787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    print(response.text)\n",
    "except:\n",
    "    print(\"No information generated by the model.\")"
   ],
   "id": "ba5fe76aff7686ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 菠萝披萨：一种对人类文明的侮辱\n",
      "\n",
      "各位爱好披萨的朋友们，你们是否也和我一样，对一种罪恶的邪恶食物感到深深的厌恶？没错，我说的就是披萨上的那颗“毒瘤”—— **菠萝**！ \n",
      "\n",
      "有些人可能还陶醉于“酸甜搭配”的幻觉，但他们忘记了：披萨是神圣的，是意大利传统美食的精华，是无数人心中最美好的味觉记忆！而菠萝，这颗生长在热带丛林的“野蛮水果”，**根本没有资格玷污这神圣的美味**！\n",
      "\n",
      "想象一下，当你在享受着香气扑鼻的披萨时，突然咬到一块**冰冷甜腻的菠萝**，那感觉就像是在天堂吃了一口**猪食**！这酸涩的口感，这奇特的搭配，简直是在挑战你的味觉极限！ \n",
      "\n",
      "更可恶的是，一些披萨店为了迎合一些“奇奇怪怪”的口味，竟然把菠萝作为一种**标准配料**，这简直是**对传统披萨的亵渎**！难道他们不知道， **披萨只有经典的美味才配得上它的名字**吗？\n",
      "\n",
      "所以，朋友们，让我们一起抵制菠萝披萨，**守护披萨的尊严！** 我们应该用更纯粹的食材，更经典的搭配，来展现披萨的魅力，而不是用这些**毫无道理的水果**来破坏它！\n",
      "\n",
      "记住，披萨是神圣的，它**不需要**任何额外的装饰，更不需要**菠萝这颗“毒瘤”**！  \n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240924175116.png)"
   ],
   "id": "4c79df5345dba37a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "为了防止一些例外，我们自己调用API时，可以将safety_setting从默认值更改为最开放的方法。并放在初始化model处，而不是单次问答中。"
   ],
   "id": "bd22aed7e31ce4a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\", \n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "]"
   ],
   "id": "be619100b05d2812"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T08:57:41.958691Z",
     "start_time": "2024-09-24T08:57:41.938691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model"
   ],
   "id": "eab0261de67408bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "啊，这，这是什么！不用担心，这些都是内置的枚举类型，我们可以跳转查看。"
   ],
   "id": "6fec24131e09cabe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T10:22:09.215193Z",
     "start_time": "2024-09-24T10:22:09.198193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "safety_settings = {\n",
    "        'hate': \"low\",\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "        'DANGEROUS': 'BLOCK_NONE',\n",
    "        \"sexually_explicit\": 1\n",
    "    }\n",
    "model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-pro\",\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "model"
   ],
   "id": "8c4b6d3991306f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-pro',\n",
       "    generation_config={},\n",
       "    safety_settings={<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_LOW_AND_ABOVE: 1>, <HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_LOW_AND_ABOVE: 1>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_LOW_AND_ABOVE: 1>},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T10:10:05.914911Z",
     "start_time": "2024-09-24T10:10:05.909912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(response)"
   ],
   "id": "903f5a8d9a878121",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Please provide me with the image or a description of what you're referring to! I need more context to determine if something looks store-bought or homemade. \\n\\nFor example, you could say:\\n\\n* \\\"These cookies look very uniform and have perfect edges. Do these look store-bought or homemade?\\\"\\n* \\\"This cake has a rustic, uneven frosting. Do these look store-bought or homemade?\\\"\\n\\nOnce you give me more information, I can help you assess the appearance and give my opinion!\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 10,\n",
      "        \"candidates_token_count\": 104,\n",
      "        \"total_token_count\": 114\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "stop_sequences\n",
    "\n",
    "停止序列允许我们提供一组字符串（最多 5 个），当Gemini在生成响应时遇到这些字符串时，就会停止生成。这是一种告诉大模型 \"如果你生成了这个序列，就停止生成其他内容\"的方法。\n",
    "如果指定，API 将在停止序列首次出现时停止。停止序列不会包含在响应中。\n",
    "\n",
    "想一想，为什么大模型要创造这个参数呢？我们可以怎样的应用呢？"
   ],
   "id": "8c25b70c1c3006ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 在这个例子中，我们设置了 } 作为停止序列。这意味着大模型将在生成 JSON 对象的右大括号后停止生成内容，从而确保我们只获得 JSON 对象本身，而不包括任何额外的解释或评论。"
   ],
   "id": "e4c252dd23222501"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:12:31.029572Z",
     "start_time": "2024-09-24T11:12:29.296925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"一句话总结朝鲜和韩国的历史与关系,要准确详细\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"stop_sequences\": [\"。\"]\n",
    "})\n",
    "print(response)"
   ],
   "id": "c3e124e8bba272ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"\\u671d\\u9c9c\\u534a\\u5c9b\\u81ea1945\\u5e74\\u5206\\u88c2\\u4ee5\\u6765\\uff0c\\u7ecf\\u5386\\u4e86\\u51b7\\u6218\\u683c\\u5c40\\u7684\\u653f\\u6cbb\\u5bf9\\u6297\\uff0c\\u4ee5\\u53ca\\u5357\\u5317\\u7edf\\u4e00\\u613f\\u671b\\u7684\\u590d\\u6742\\u7ea0\\u845b\\uff0c\\u81f3\\u4eca\\u4ecd\\u5904\\u4e8e\\u5206\\u6cbb\\u72b6\\u6001\\uff0c\\u5173\\u7cfb\\u7d27\\u5f20\\u4e14\\u5145\\u6ee1\\u4e0d\\u786e\\u5b9a\\u6027\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 13,\n",
      "        \"candidates_token_count\": 44,\n",
      "        \"total_token_count\": 57\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:12:56.177900Z",
     "start_time": "2024-09-24T11:12:56.159902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response.text"
   ],
   "id": "1d15af23d7358b8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'朝鲜半岛自1945年分裂以来，经历了冷战格局的政治对抗，以及南北统一愿望的复杂纠葛，至今仍处于分治状态，关系紧张且充满不确定性'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:16:22.352072Z",
     "start_time": "2024-09-24T11:16:20.902312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"列出三种水果，每种水果用<fruit>标签包围。\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"stop_sequences\": [\"</fruit>\"]\n",
    "})\n",
    "print(response)"
   ],
   "id": "e0a6ac11b90ac04c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"<fruit>\\u82f9\\u679c\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 16,\n",
      "        \"candidates_token_count\": 4,\n",
      "        \"total_token_count\": 20\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T11:16:23.607272Z",
     "start_time": "2024-09-24T11:16:23.591272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response.text"
   ],
   "id": "da323e790deedacd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<fruit>苹果'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Json模式——结构化模型的输出\n",
    "\n",
    "可以在提示中设置所需的结构化模型的输出，gemini也单独为模型提供了架构与参数。\n",
    "\n",
    "数据提取和分析： 在这种场景中，结构化输出可以直接被用于数据分析或数据库插入。\n",
    "内容生成： 在内容生成任务中，结构化输出可以帮助控制生成内容的格式和结构。例如，生成一篇带有特定结构的文章：\n",
    "\n",
    "我们先看看在提示中设置所需的结构化模型的输出"
   ],
   "id": "7b85c75bc9710a9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"写一篇关于气候变化的短文，包含以下部分：<introduction>引言</introduction><body>正文</body><conclusion>结论</conclusion>\"}\n",
    "]"
   ],
   "id": "27c52763b47cb588"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"提供一个5步计划来学习一门新语言。每个步骤都应包含<step>标题</step>和<description>描述</description>。\"}\n",
    "]"
   ],
   "id": "eccd8111d218b4f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "要求模型生成特定数量的问答对，并可以分隔问题和答案，"
   ],
   "id": "e9c45e3715408ea5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"请生成关于人工智能的问答对。使用以下格式：\\n1. Q: [问题]\\nA: [答案]\\n2. Q: [问题]\\nA: [答案]\\n3. Q: [问题]\\nA: [答案]\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"stop_sequences\": [\"</fruit>\"]\n",
    "})\n",
    "print(response)"
   ],
   "id": "819555cf3f8bd528"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "prompt = \"生成3个关于人工智能的问答对，以JSON格式输出。格式如下：\\n[{\\\"question\\\": \\\"问题1\\\", \\\"answer\\\": \\\"答案1\\\"}, {\\\"question\\\": \\\"问题2\\\", \\\"answer\\\": \\\"答案2\\\"}, {\\\"question\\\": \\\"问题3\\\", \\\"answer\\\": \\\"答案3\\\"}]\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"stop_sequences\": [\"</fruit>\"]\n",
    "})\n",
    "qa_pairs = json.loads(response.content[0].text)\n",
    "for pair in qa_pairs:\n",
    "    print(f\"Q: {pair['question']}\")\n",
    "    print(f\"A: {pair['answer']}\\n\")"
   ],
   "id": "ad9f301a90efb60c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "prompt = \"生成3个关于人工智能的问答对，使用HTML标签格式。例如：<qa><q>问题</q><a>答案</a></qa>\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"stop_sequences\": [\"</fruit>\"]\n",
    "})\n",
    "soup = BeautifulSoup(response.content[0].text, 'html.parser')\n",
    "qa_pairs = soup.find_all('qa')\n",
    "for pair in qa_pairs:\n",
    "    print(f\"Q: {pair.q.text}\")\n",
    "    print(f\"A: {pair.a.text}\\n\")"
   ],
   "id": "f97bfb6aeb5cca55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"请生成关于人工智能的问答对。使用以下格式：\\n1. Q: [问题]\\nA: [答案]\\n2. Q: [问题]\\nA: [答案]\\n3. Q: [问题]\\nA: [答案]\"\n",
    "response = model.generate_content(contents=prompt, generation_config={\n",
    "    \"stop_sequences\": [\"</fruit>\"]\n",
    "})\n",
    "print(response)"
   ],
   "id": "d26fc447227028b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_gemini_client():\n",
    "    generation_config = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 1,\n",
    "        \"max_output_tokens\": 2048,\n",
    "    }\n",
    "\n",
    "    safety_settings = [\n",
    "        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "            \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "            \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "            \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-pro\",\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "    )\n",
    "    client = model.start_chat()\n",
    "    return client\n",
    "\n",
    "\n",
    "def pro_prompt_by_gemini(prompt: str, client) -> str:\n",
    "    # TODO fix the type hint\n",
    "    prompt = f\"revise `{prompt}` to a DALL-E prompt, return the content in English only return the scene and detail\"\n",
    "    client.send_message(prompt)\n",
    "    return client.last.text\n",
    "\n",
    "\n",
    "gemini_client = make_gemini_client()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e21241acac9312f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Google特有的Json的格式\n",
    "最新的模型（1.5 及更高版本）允许您直接传递模式对象（或等效的 python 类型），并且输出将严格遵循该模式。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb140455eed59af4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过在generation_config参数中指定respose_mime_type来激活 JSON 模式："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5a2d4181724f069"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\",\n",
    "                              generation_config={\"response_mime_type\": \"application/json\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19e34d09ad05867b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "result = model.generate_content(\n",
    "    [\"What kind of instrument is this:\", organ],\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema={\n",
    "            \"type\": \"STRING\",\n",
    "            \"enum\": [\"Percussion\", \"String\", \"Woodwind\", \"Brass\", \"Keyboard\"],\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "print(result)  # Keyboard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "681d6d7e04a80879"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import enum\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class Grade(enum.Enum):\n",
    "    A_PLUS = \"a+\"\n",
    "    A = \"a\"\n",
    "    B = \"b\"\n",
    "    C = \"c\"\n",
    "    D = \"d\"\n",
    "    F = \"f\"\n",
    "\n",
    "class Recipe(TypedDict):\n",
    "    recipe_name: str\n",
    "    grade: Grade\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "result = model.generate_content(\n",
    "    \"List about 10 cookie recipes, grade them based on popularity\",\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"application/json\", response_schema=list[Recipe]\n",
    "    ),\n",
    ")\n",
    "print(result)  # [{\"grade\": \"a+\", \"recipe_name\": \"Chocolate Chip Cookies\"}, ...]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2439dd11d31c9682"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
