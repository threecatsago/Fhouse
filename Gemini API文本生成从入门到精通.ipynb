{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP0XjKM/h9JsdykKOPWamxF"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gemini API基本调用——文本生成\n",
    "\n",
    "环境准备：安装依赖库和设置API KEY"
   ],
   "metadata": {
    "id": "fQ1pK4fVln8g"
   }
  },
  {
   "cell_type": "markdown",
   "source": "适用于 Gemini API 的 Python SDK 包含在 google-generativeai 软件包。使用 pip 安装：\n",
   "metadata": {
    "id": "S1hL7ScGtx7U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q -U google-generativeai"
   ],
   "metadata": {
    "id": "pmFQUZiut12C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "如需使用 Gemini API，是需要注册gemini后免费获取调用 API 密钥的。如果您还没有账号， 需要在 Google AI Studio 中注册创建密钥（在前面的课程内容中）。"
   ],
   "metadata": {
    "id": "al3WqEncuh4x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gemini 1.5 Flash 和 Gemini 1.5 Pro 免费层级的功能及价格\n",
    "\n",
    "| 功能 | Gemini 1.5 Flash | Gemini 1.5 Pro |\n",
    "|---|---|---|\n",
    "| 模型 | Gemini 1.5 Flash | Gemini 1.5 Pro |\n",
    "| 特点 | 最快的多模态模型，处理重复性任务出色，100万个上下文窗口 | 新一代模型，突破性的 200 万上下文窗口 |\n",
    "| 发布状态 | 正式推出，可用于生产环境 | 正式推出，可用于生产环境 |\n",
    "| **免费层级** |  |  |\n",
    "| API 服务 | 通过 API 服务提供，速率限制较低，用于测试目的 | 通过 API 服务提供，速率限制较低，用于测试目的 |\n",
    "| Google AI Studio | 在所有适用的国家/地区完全免费 | 在所有适用的国家/地区完全免费 |\n",
    "| **速率限制** |  |  |\n",
    "| 每分钟请求数 (RPM) | 15 | 2 |\n",
    "| 每分钟令牌数 (TPM) | 1,000,000 | 32,000 |\n",
    "| 每日请求数 (RPD) | 1,500 | 50 |\n",
    "| **价格** |  |  |\n",
    "| 输入价格 | 免费 | 免费 |\n",
    "| 输出价格 | 免费 | 免费 |\n",
    "| **上下文缓存** |  |  |\n",
    "| 存储 | 免费，每小时最多 100 万个令牌存储 | 不适用 |\n",
    "\n",
    "**总结:**\n",
    "\n",
    "* Gemini 1.5 Flash 更适合需要高吞吐量和较大上下文窗口（100 万）的任务，例如处理重复性任务。\n",
    "* Gemini 1.5 Pro 提供更大的上下文窗口（200 万），但免费层级的速率限制较低。它更适合需要处理更长文本或对话的场景，但目前不支持调整和上下文缓存。\n"
   ],
   "metadata": {
    "id": "9dymNbmQ-jrh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入库并配置APIKEY。"
   ],
   "metadata": {
    "id": "FgzlDNzCuC8o"
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "id": "obpZJtjj5K3I"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qiooAraIeRzh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726718952184,
     "user_tz": -480,
     "elapsed": 1284,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-09-26T10:41:45.665052Z",
     "start_time": "2024-09-26T10:41:45.653052Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 第一种方法，通过环境变量隐式的存储，\n",
    "# 会在我的电脑里面寻找名为\"API_KEY\"的环境变量，\n",
    "# 所以可以将APIKEY由环境变量的方式存在于电脑中，如果只是在自己电脑上使用，推荐第一种\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"], transport='rest')\n",
    "\n",
    "# 第二种方法，在电脑中显性的用变量显示\n",
    "# genai.configure(api_key=\"AIza...\",transport='rest')\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "发出第一个请求"
   ],
   "metadata": {
    "id": "TAESYTYUuMAT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
    "response = model.generate_content(\"响指是如何打响的\")\n",
    "print(response.text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "3k0XR6YHeVI-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726672982591,
     "user_tz": -480,
     "elapsed": 5783,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "8c4b0e9e-f3d7-4e03-f589-ac18230802ee",
    "ExecuteTime": {
     "end_time": "2024-09-19T10:22:30.849529Z",
     "start_time": "2024-09-19T10:22:27.677687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "响指的原理其实很简单，它利用的是 **弹性势能** 的释放。\n",
      "\n",
      "1. **压迫：** 当你将拇指和食指的指尖紧贴在一起，并在拇指的侧面施加压力时，你实际上是压缩了食指的指尖。\n",
      "2. **存储弹性势能：** 食指的指尖就像一个被压缩的弹簧一样，存储了弹性势能。\n",
      "3. **突然释放：** 当你迅速地将食指和拇指分开时，你突然释放了存储的弹性势能。\n",
      "4. **空气振动：** 由于能量的释放，食指的指尖会快速移动，并击打空气，从而产生一个压力波。\n",
      "5. **声音：** 这个压力波会迅速传播，并最终传到你的耳朵，让你听到响指的声音。\n",
      "\n",
      "简单来说，响指就是利用指尖的压缩和释放，产生一个快速移动的物体（指尖）来击打空气，从而发出声音。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "可能遇到的报错由于每个人电脑的Python环境不同,有可能出现相关库的版本过低等，则可以重新pip install安装\n",
    "\n",
    "如果是网络超时的问题，则检查要设置网络传输方式transport = ‘rest’，即ganai配置函数中genai.configure（）添加transport = ‘rest’"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 掌握Gemini API 的coding\n",
    "###  1. Gemini API 之文本调用"
   ],
   "metadata": {
    "id": "dlVk5wMO6kpo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gemini API 支持文本，图像，Vision，音频，长上下文，代码执行，JSON 模式，函数调用，系统指令等生成功能。我们先从文本生成开始讲起。\n",
    "\n",
    "它的主要流程是这样：\n",
    "\n",
    "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240919153231.png)\n",
    "\n",
    "创建genai实例并配置API KEY，genai实例选择模型生成模型实例，模型实例调用generateContent函数生成对话"
   ],
   "metadata": {
    "id": "pLle7pZlvitC"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "以下两行代码就创建genai实例并配置了API KEY"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:48:51.397357Z",
     "start_time": "2024-09-26T10:48:51.385357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "通过API Key认证的genai就和我们的账户挂钩了，同时也建立了调用Gemini的桥梁，genai实例就相当于我们代码版的 ai studio，能关联到我们的账户，比如我们的文件，微调的模型等。在这里我们，可以理解它为和我们自己滴血结成契约的独有小精灵，麾下能召唤许多模型们，各自的模型小助手们能够联络Gemini模型并生成回答。"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "genai实例并不能直接生成回答，而是genai实例选择模型生成模型实例才可以调用函数得到回答。genai实例更像是更上一层级的账户管理，即导航菜单。\n",
    "\n",
    "例如它可以存储文件，查看模型等等。其文件 API 允许每个项目存储最多 20GB 的文件，每个文件的大小不超过 2GB。文件存储 48 小时，并且可以在此期间使用您的 API 密钥进行访问，服务为免费提供。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:48:54.139713Z",
     "start_time": "2024-09-26T10:48:54.125712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProject\\Gemini-系列\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:52:06.203798Z",
     "start_time": "2024-09-26T10:48:56.416827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "sample_file = genai.upload_file(path=\"media/压缩_手写照片.jpg\", display_name=\"Sample drawing\")"
   ],
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTimeoutError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32mD:\\Temp\\ipykernel_165912\\2377556897.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0msample_file\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgenai\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupload_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"images/压缩_手写照片.jpg\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisplay_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"Sample drawing\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\generativeai\\files.py\u001B[0m in \u001B[0;36mupload_file\u001B[1;34m(path, mime_type, name, display_name, resumable)\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[0mdisplay_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 71\u001B[1;33m     response = client.create_file(\n\u001B[0m\u001B[0;32m     72\u001B[0m         \u001B[0mpath\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmime_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmime_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisplay_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdisplay_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresumable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mresumable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     73\u001B[0m     )\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\generativeai\\client.py\u001B[0m in \u001B[0;36mcreate_file\u001B[1;34m(self, path, mime_type, name, display_name, resumable, metadata)\u001B[0m\n\u001B[0;32m     98\u001B[0m     ) -> protos.File:\n\u001B[0;32m     99\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_discovery_api\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 100\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_setup_discovery_api\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    101\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    102\u001B[0m         \u001B[0mfile\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\generativeai\\client.py\u001B[0m in \u001B[0;36m_setup_discovery_api\u001B[1;34m(self, metadata)\u001B[0m\n\u001B[0;32m     79\u001B[0m             \u001B[0mheaders\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m         )\n\u001B[1;32m---> 81\u001B[1;33m         \u001B[0mresponse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrequest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     82\u001B[0m         \u001B[0mrequest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhttp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\googleapiclient\\_helpers.py\u001B[0m in \u001B[0;36mpositional_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    128\u001B[0m                 \u001B[1;32melif\u001B[0m \u001B[0mpositional_parameters_enforcement\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mPOSITIONAL_WARNING\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    129\u001B[0m                     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    131\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    132\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mpositional_wrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\googleapiclient\\http.py\u001B[0m in \u001B[0;36mexecute\u001B[1;34m(self, http, num_retries)\u001B[0m\n\u001B[0;32m    921\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    922\u001B[0m         \u001B[1;31m# Handle retries for server-side errors.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 923\u001B[1;33m         resp, content = _retry_request(\n\u001B[0m\u001B[0;32m    924\u001B[0m             \u001B[0mhttp\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    925\u001B[0m             \u001B[0mnum_retries\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\googleapiclient\\http.py\u001B[0m in \u001B[0;36m_retry_request\u001B[1;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001B[0m\n\u001B[0;32m    220\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mexception\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mretry_num\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mnum_retries\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 222\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mexception\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    223\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m                 \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\googleapiclient\\http.py\u001B[0m in \u001B[0;36m_retry_request\u001B[1;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    190\u001B[0m             \u001B[0mexception\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 191\u001B[1;33m             \u001B[0mresp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhttp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0muri\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    192\u001B[0m         \u001B[1;31m# Retry on SSL errors and socket timeout errors.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    193\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0m_ssl_SSLError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mssl_error\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\httplib2\\__init__.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(self, uri, method, body, headers, redirections, connection_type)\u001B[0m\n\u001B[0;32m   1722\u001B[0m                     \u001B[0mcontent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mb\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1723\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1724\u001B[1;33m                     (response, content) = self._request(\n\u001B[0m\u001B[0;32m   1725\u001B[0m                         \u001B[0mconn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mauthority\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muri\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequest_uri\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mredirections\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcachekey\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1726\u001B[0m                     )\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\httplib2\\__init__.py\u001B[0m in \u001B[0;36m_request\u001B[1;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001B[0m\n\u001B[0;32m   1442\u001B[0m             \u001B[0mauth\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequest_uri\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1443\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1444\u001B[1;33m         \u001B[1;33m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontent\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_conn_request\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequest_uri\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1445\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1446\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mauth\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\httplib2\\__init__.py\u001B[0m in \u001B[0;36m_conn_request\u001B[1;34m(self, conn, request_uri, method, body, headers)\u001B[0m\n\u001B[0;32m   1364\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1365\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mconn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msock\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1366\u001B[1;33m                     \u001B[0mconn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1367\u001B[0m                 \u001B[0mconn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrequest_uri\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1368\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\httplib2\\__init__.py\u001B[0m in \u001B[0;36mconnect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1200\u001B[0m             \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1201\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1202\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0msocket_err\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1203\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1204\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\httplib2\\__init__.py\u001B[0m in \u001B[0;36mconnect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1154\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mhas_timeout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1155\u001B[0m                     \u001B[0msock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msettimeout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1156\u001B[1;33m                 \u001B[0msock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhost\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mport\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1157\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1158\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msock\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_context\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrap_socket\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msock\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mserver_hostname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhost\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTimeoutError\u001B[0m: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3cqE5-9yG0o",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726725240214,
     "user_tz": -480,
     "elapsed": 598,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "43da9ae1-cafe-4362-a091-774456644ffa"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Uploaded file 'Sample drawing' as: https://generativelanguage.googleapis.com/v1beta/files/kkfb3ho0yt42\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "genai.list_files()\n",
    "\n",
    "import pprint\n",
    "\n",
    "for file_path in genai.list_files():\n",
    "    pprint.pprint(file)"
   ],
   "metadata": {
    "id": "Sk5uwxEJAow5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726725249030,
     "user_tz": -480,
     "elapsed": 2638,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "outputId": "a2214175-48ac-473e-dcb3-5f7eb425d99a",
    "ExecuteTime": {
     "end_time": "2024-09-26T10:41:51.443407Z",
     "start_time": "2024-09-26T10:41:50.238363Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": "这个genai实例也可以列出可以在Gemini API 中可以使用的模型，并查找有关模型的详细信息。",
   "metadata": {
    "id": "XxH7KdDw-sEb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "genai.list_models()\n",
    "\n",
    "import pprint\n",
    "\n",
    "for model in genai.list_models():\n",
    "    pprint.pprint(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qePF6PK549B8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726675792564,
     "user_tz": -480,
     "elapsed": 3877,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "496ea0d4-e701-4058-cda1-5c0328426042"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Latest',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
      "                   'model.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
      "                   'model that supports tuning.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=0.9,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro 001',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-exp-0801',\n",
      "      base_model_id='',\n",
      "      version='exp-0801',\n",
      "      display_name='Gemini 1.5 Pro Experimental 0801',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-pro-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-0827',\n",
      "      display_name='Gemini 1.5 Pro Experimental 0827',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2097152,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-001-tuning',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=16384,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='exp-0827',\n",
      "      display_name='Gemini 1.5 Flash Experimental 0827',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "使用list_models()查看可用的模型。这些模型要支持generateContent，这是我们本次讲的用于对话，模型生成回答的主要方法。",
   "metadata": {
    "id": "32nCwd5o-3Yo"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "source": [
    "for m in genai.list_models():\n",
    "    if \"generateContent\" in m.supported_generation_methods:\n",
    "        print(m.name)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "UWrlrvZ_-4rA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726727614591,
     "user_tz": -480,
     "elapsed": 3295,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "9ca065e8-ef6e-4373-acd8-a3566923119e"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0827\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "有了独特的的契约助手（由我们API KEY认证的）后，如果我们想要向大模型发起问话，需要什么必须的信息告诉助手呢？\n",
    "\n",
    "1、 使用哪个模型，我们也已经获取了模型列表 2、我们的问题是什么\n",
    "\n",
    "具体的使用方式是先通过GenerativeModel()创建实例,里面传入指定要使用的模型名称,格式为 \"models/{model}\"，就创建了模型的一个实例，撒豆成兵"
   ],
   "metadata": {
    "id": "M0-jAQRZBdDc"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "source": [
    "model = genai.GenerativeModel('models/gemini-1.5-flash-exp-0827')"
   ],
   "metadata": {
    "id": "VvroOxpV4PpA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726727424854,
     "user_tz": -480,
     "elapsed": 576,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_info = genai.get_model('models/gemini-1.5-flash-exp-0827')\n",
    "model_info"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "_BB-TrBb-Q3T",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726728086265,
     "user_tz": -480,
     "elapsed": 3095,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "1154a8a0-b5c9-4750-9577-5cd2c259709a"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Model(name='models/gemini-1.5-flash-exp-0827',\n",
       "      base_model_id='',\n",
       "      version='exp-0827',\n",
       "      display_name='Gemini 1.5 Flash Experimental 0827',\n",
       "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "      input_token_limit=1048576,\n",
       "      output_token_limit=8192,\n",
       "      supported_generation_methods=['generateContent', 'countTokens'],\n",
       "      temperature=1.0,\n",
       "      max_temperature=2.0,\n",
       "      top_p=0.95,\n",
       "      top_k=64)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "model.generate_content()",
   "metadata": {
    "id": "qtN95VgpCfi1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "generate_content函数是Gemini API中的重要方法,用于生成模型响应，可以用于各种任务,如文本生成、对话、音频、视频、调用微调后的模型发起对话等",
   "metadata": {
    "id": "Hi9ogU7lCr2q"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "因为是在模型实例下的方法，这个对象本身就包含了model的信息，所以函数里面只需要把对话信息加上就可以了\n",
    "\n",
    "主要参数:\n",
    "\n",
    "| 参数 | 说明 | 是否必需 |\n",
    "|---|---|---|\n",
    "| contents | 包含对话内容的列表。单轮对话包含一个元素，多轮对话包含整个历史 | 是 |\n",
    "| tools | 允许模型使用外部工具的列表，支持 Function 和 codeExecution | 否 |\n",
    "| toolConfig | 工具的配置信息 | 否 |\n",
    "| safetySettings | 安全设置列表，用于阻止不安全内容 | 否 |\n",
    "| systemInstruction | 设置的系统指令，目前仅支持文本 | 否 |\n",
    "| generationConfig | 模型生成和输出的 token 配置选项 | 否 |\n",
    "| cachedContent | 使用缓存的内容作为上下文 | 否 | \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "response = model.generate_content(\"响指是如何打响的\")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "id": "kgfGH4bPCN84",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726678415469,
     "user_tz": -480,
     "elapsed": 11993,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "bc307a71-138e-46d4-a327-a6358e660d1d",
    "ExecuteTime": {
     "end_time": "2024-09-19T10:37:36.629199Z",
     "start_time": "2024-09-19T10:37:33.953408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"\\u54cd\\u6307\\u7684\\u539f\\u7406\\u5728\\u4e8e\\u5229\\u7528\\u624b\\u6307\\u7684\\u5feb\\u901f\\u8fd0\\u52a8\\u548c\\u7a7a\\u6c14\\u6469\\u64e6\\u4ea7\\u751f\\u7684\\u58f0\\u97f3\\u3002 \\n\\n\\u5177\\u4f53\\u6765\\u8bf4\\uff1a\\n\\n1. **\\u5feb\\u901f\\u79fb\\u52a8:** \\u5f53\\u4f60\\u5feb\\u901f\\u5730\\u5c06\\u62c7\\u6307\\u548c\\u98df\\u6307\\u7528\\u529b\\u6469\\u64e6\\u5728\\u4e00\\u8d77\\u65f6\\uff0c\\u5b83\\u4eec\\u4e4b\\u95f4\\u7684\\u7a7a\\u9699\\u8fc5\\u901f\\u53d8\\u5c0f\\uff0c\\u5e76\\u8feb\\u4f7f\\u7a7a\\u6c14\\u4ece\\u8fd9\\u4e2a\\u7a7a\\u9699\\u4e2d\\u5feb\\u901f\\u6d41\\u51fa\\u3002\\n2. **\\u7a7a\\u6c14\\u6469\\u64e6:** \\u7531\\u4e8e\\u7a7a\\u6c14\\u4ece\\u72ed\\u5c0f\\u7684\\u7a7a\\u95f4\\u4e2d\\u5feb\\u901f\\u6d41\\u51fa\\uff0c\\u5b83\\u4f1a\\u4e0e\\u5468\\u56f4\\u7684\\u7a7a\\u6c14\\u4ea7\\u751f\\u5267\\u70c8\\u7684\\u6469\\u64e6\\uff0c\\u4ece\\u800c\\u4ea7\\u751f\\u9ad8\\u9891\\u7684\\u58f0\\u6ce2\\u3002\\n3. **\\u58f0\\u97f3\\u653e\\u5927:** \\u4f60\\u7684\\u624b\\u638c\\u548c\\u624b\\u6307\\u5f62\\u6210\\u4e86\\u4e00\\u4e2a\\u5171\\u9e23\\u8154\\uff0c\\u5c06\\u8fd9\\u4e9b\\u9ad8\\u9891\\u58f0\\u6ce2\\u653e\\u5927\\uff0c\\u4ece\\u800c\\u4f7f\\u4f60\\u542c\\u5230\\u54cd\\u4eae\\u7684\\u201c\\u556a\\u201d\\u7684\\u4e00\\u58f0\\u3002\\n\\n\\u6b64\\u5916\\uff0c\\u54cd\\u6307\\u7684\\u58f0\\u97f3\\u8fd8\\u53d7\\u5230\\u4ee5\\u4e0b\\u56e0\\u7d20\\u7684\\u5f71\\u54cd\\uff1a\\n\\n* **\\u624b\\u6307\\u7684\\u529b\\u5ea6\\u548c\\u901f\\u5ea6:** \\u529b\\u91cf\\u548c\\u901f\\u5ea6\\u8d8a\\u5feb\\uff0c\\u54cd\\u6307\\u7684\\u58f0\\u97f3\\u5c31\\u8d8a\\u54cd\\u4eae\\u3002\\n* **\\u624b\\u6307\\u7684\\u6469\\u64e6\\u7cfb\\u6570:** \\u6469\\u64e6\\u7cfb\\u6570\\u8d8a\\u9ad8\\uff0c\\u58f0\\u97f3\\u5c31\\u8d8a\\u54cd\\u4eae\\u3002\\n* **\\u5468\\u56f4\\u73af\\u5883:** \\u5728\\u5b89\\u9759\\u7684\\u73af\\u5883\\u4e2d\\uff0c\\u54cd\\u6307\\u7684\\u58f0\\u97f3\\u4f1a\\u66f4\\u660e\\u663e\\u3002\\n\\n\\u6240\\u4ee5\\uff0c\\u54cd\\u6307\\u5e76\\u4e0d\\u662f\\u7b80\\u5355\\u7684\\u6572\\u51fb\\uff0c\\u800c\\u662f\\u5229\\u7528\\u624b\\u6307\\u7684\\u5feb\\u901f\\u8fd0\\u52a8\\u548c\\u7a7a\\u6c14\\u6469\\u64e6\\u4ea7\\u751f\\u7684\\u58f0\\u97f3\\u3002\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 7,\n",
      "        \"candidates_token_count\": 233,\n",
      "        \"total_token_count\": 240\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": "print(response.text)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "id": "qD6cz6IXdrUV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726718994366,
     "user_tz": -480,
     "elapsed": 4505,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "f8de2fe6-d63c-4bc6-c2c4-638611c6cfc5",
    "ExecuteTime": {
     "end_time": "2024-09-19T07:53:34.055525Z",
     "start_time": "2024-09-19T07:53:34.043567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打响指的原理是利用手指的快速运动和空气压力差来产生响声。\n",
      "\n",
      "**具体步骤：**\n",
      "\n",
      "1. **伸直食指和拇指。**\n",
      "2. **将食指紧贴拇指的侧面。**\n",
      "3. **快速弹动食指，使其离开拇指并突然释放。**\n",
      "\n",
      "**原理：**\n",
      "\n",
      "* 当食指快速弹动时，它会压缩拇指和食指之间的空气。\n",
      "* 由于空气是不可压缩的，这种压缩会产生一个高压区域。\n",
      "* 当食指离开拇指时，高压区域会迅速膨胀，导致空气压力突然降低。\n",
      "* 这个突然的压力变化会在空气中产生声波，我们听到的就是响指的声音。\n",
      "\n",
      "**影响响指声音的因素：**\n",
      "\n",
      "* **手指的力量和速度：**手指弹动的力量和速度越大，响指的声音越响亮。\n",
      "* **手指的形状和大小：**手指的形状和大小会影响空气压缩的程度，从而影响响指的声音。\n",
      "* **周围环境：**周围环境的温度和湿度也会影响响指的声音。\n",
      "\n",
      "**其他：**\n",
      "\n",
      "* 打响指需要一定的技巧和练习才能掌握。\n",
      "* 响指是一种常见的表达方式，可以用来表达各种情绪，例如赞赏、兴奋、惊讶等等。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240919153231.png)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T07:54:25.412328Z",
     "start_time": "2024-09-19T07:54:22.648652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"], transport='rest')\n",
    "model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
    "response = model.generate_content(\"影子可不可以是彩色的\")\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "影子可以是彩色的！虽然我们通常认为影子是黑色的，但实际上，它们只是光线不足的地方。  \n",
      "\n",
      "当光线穿过有色物体时，它会吸收某些颜色并反射其他颜色。如果光线照射到红色物体上，红色物体将吸收大部分光线并反射红色光线。当这种红色光线照射到表面时，就会形成一个红色的影子。\n",
      "\n",
      "所以，如果光源是彩色光，或者物体是透明的且有颜色，那么影子就会呈现出相应的光线或物体的颜色。例如，如果用蓝色灯光照射一个红色的物体，影子将呈现紫色。\n",
      "\n",
      "以下是影子呈现彩色的几种情况：\n",
      "\n",
      "* **彩色光源：**使用彩色灯光照射物体，例如舞台灯光、霓虹灯等。\n",
      "* **透明彩色物体：**例如，彩色玻璃、彩色水等。\n",
      "* **光线折射：**光线穿过不同介质时会发生折射，例如透过棱镜的光线。\n",
      "\n",
      "因此，影子并非只有黑色，它们也可以呈现出各种各样的颜色，这取决于光线和物体的特性。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在框架我们知道了解了，来细究一下负责对话构建的也是唯一的一个必须参数——contents"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "主要参数:\n",
    "\n",
    "| 参数 | 说明 | 是否必需 |\n",
    "|---|---|---|\n",
    "| contents | 包含对话内容的列表。单轮对话包含一个元素，多轮对话包含整个历史 | 是 |\n",
    "| tools | 允许模型使用外部工具的列表，支持 Function 和 codeExecution | 否 |\n",
    "| toolConfig | 工具的配置信息 | 否 |\n",
    "| safetySettings | 安全设置列表，用于阻止不安全内容 | 否 |\n",
    "| systemInstruction | 设置的系统指令，目前仅支持文本 | 否 |\n",
    "| generationConfig | 模型生成和输出的 token 配置选项 | 否 |\n",
    "| cachedContent | 使用缓存的内容作为上下文 | 否 | \n",
    "\n",
    "\n",
    "对话构建：`contents: List[Dict[str, Any]]`\n",
    "\n",
    "**定义**：包含对话历史和当前用户输入的消息列表。\n",
    "\n",
    "**格式**：列表，包含字典，字典中的键的类型是 str（字符串类型），字典中的值可以是任意类型。\n",
    "\n",
    "**重要性**：\n",
    "- 提供对话上下文\n",
    "- 决定 AI 理解和回应的基础\n",
    "\n",
    "列表里的Dict[str, Any]对应一个Content对象，\n",
    "\n",
    "1. Content 结构:\n",
    "   Content 主要由两个字段组成:parts 和 role。\n",
    "\n",
    "2. role 字段:\n",
    "   - 可选字段,表示内容的生产者是谁，如果是单轮对话,可以留空不设置\n",
    "   - 值只能是 'user' 或 'model'\n",
    "   - 在多轮对话中很有用,用于区分是用户输入还是模型回复\n",
    "\n",
    "3. parts 字段:\n",
    "   这是一个包含 Part 对象的列表,每个 Part 代表消息的一部分。对于文本相关的内容,我们主要关注 Part 中的 text 字段。即是一个字符串类型的字段，就是用户的问题或模型的回答。\n",
    "\n",
    "用法格式：\n",
    "\n",
    "   ```python\n",
    "   messages = [\n",
    "       {\n",
    "           'role': 'user',\n",
    "           'parts': ['你的消息文本']\n",
    "       }\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "\n",
    "4. 完整的使用示例：\n",
    "   ```python\n",
    "   # 单轮对话\n",
    "   messages = [{'role': 'user', 'parts': ['你好，我是羚伊']}]\n",
    "   response = model.generate_content(messages)\n",
    "\n",
    "   print(response.text)\n",
    "\n",
    "   # 多轮对话\n",
    "   messages = [\n",
    "       {'role': 'user', 'parts': ['你好，我是羚伊']},\n",
    "       {'role': 'model', 'parts': ['你好羚伊！很高兴认识你。我是一个AI助手']},\n",
    "       {'role': 'user', 'parts': ['我叫什么名字']}\n",
    "   ]\n",
    "   response = model.generate_content(messages)\n",
    "\n",
    "   print(response.text)\n",
    "   ```\n",
    "   - 对于多轮对话，只需要在messages列表中添加更多的字典（上下文消息）即可。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:47:52.229363Z",
     "start_time": "2024-09-19T10:47:50.904496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'parts': ['你好，我是羚伊']},\n",
    "    {'role': 'model', 'parts': ['你好羚伊！很高兴认识你。我是一个AI助手，有什么我可以帮助你的吗？']},\n",
    "    {'role': 'user', 'parts': ['我叫什么名字']}\n",
    "]\n",
    "response = model.generate_content(messages)\n",
    "\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你说你叫羚伊呀 是不是想确认一下？ \n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  2. 多轮对话——构建智能交互系统\n",
    "怎样完成人类用户和 AI 助手之间的来回交互的多轮对话呢？以下是一些关键点：\n",
    "\n",
    "#### 对话历史\n",
    "\n",
    "- API 是无状态的，意味着每次请求都需要发送完整的对话历史。\n",
    "- 较早的对话轮次不一定需要实际来自大模型 - 可以使用合成的 model 消息。\n",
    "\n",
    "#### 揭秘对话结构\n",
    "\n",
    "##### 对话的消息排列规则\n",
    "\n",
    "- 对话在 `user` 和 `model` 之间交替。\n",
    "- 第一条消息必须是 `user` 角色。\n",
    "\n",
    "##### 对话中每条消息的格式\n",
    "\n",
    "- 每条消息都有 `role` 和 `parts`, 即完整的消息格式。\n",
    "-  `role`只有 `user` 或 `model`。\n",
    "- `parts`内容可以是文本或其他类型（如图像，函数调用的返回响应等）。\n",
    "\n",
    "重点是向对话消息中留存添加我们每次的对话历史。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "messages = [{'role': 'user', 'parts': ['你好，我是羚伊']}]\n",
    "response = model.generate_content(messages)"
   ],
   "metadata": {
    "id": "21pmxf0CsSVc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726722705582,
     "user_tz": -480,
     "elapsed": 3877,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-09-19T10:58:12.232411Z",
     "start_time": "2024-09-19T10:58:10.708439Z"
    }
   },
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "print(response.text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpmN0j6Tsbk2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726722722366,
     "user_tz": -480,
     "elapsed": 590,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "012ec020-67df-4837-f9f4-f6407bf882fe",
    "ExecuteTime": {
     "end_time": "2024-09-19T10:50:22.598679Z",
     "start_time": "2024-09-19T10:50:22.583732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，羚伊！很高兴认识你。 你今天过得怎么样？你想聊些什么呢？ \n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:50:48.642586Z",
     "start_time": "2024-09-19T10:50:48.635609Z"
    }
   },
   "cell_type": "code",
   "source": "print(response)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"\\u4f60\\u597d\\uff0c\\u7f9a\\u4f0a\\uff01\\u5f88\\u9ad8\\u5174\\u8ba4\\u8bc6\\u4f60\\u3002 \\u4f60\\u4eca\\u5929\\u8fc7\\u5f97\\u600e\\u4e48\\u6837\\uff1f\\u4f60\\u60f3\\u804a\\u4e9b\\u4ec0\\u4e48\\u5462\\uff1f \\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 6,\n",
      "        \"candidates_token_count\": 21,\n",
      "        \"total_token_count\": 27\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:58:37.917038Z",
     "start_time": "2024-09-19T10:58:37.907923Z"
    }
   },
   "cell_type": "code",
   "source": "print(response.candidates[0].content)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts {\n",
      "  text: \"你好，羚伊！很高兴认识你。 \\n\\n你想聊些什么呢？ \\n\\n我可以帮你：\\n\\n*  找到信息 \\n*  写故事 \\n*  翻译语言 \\n*  玩游戏 \\n\\n或者，你也可以跟我聊任何你感兴趣的话题。 \\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "source": [
    "messages.append(response.candidates[0].content)\n",
    "print(messages)"
   ],
   "metadata": {
    "id": "S2IqjMslse5q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726722780614,
     "user_tz": -480,
     "elapsed": 4887,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-09-19T10:52:39.844127Z",
     "start_time": "2024-09-19T10:52:39.832189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'parts': ['你好，我是羚伊']}, parts {\n",
      "  text: \"你好，羚伊！很高兴认识你。 你今天过得怎么样？你想聊些什么呢？ \\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:53:11.667895Z",
     "start_time": "2024-09-19T10:53:08.749327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages.append({'role': 'user', 'parts': ['月亮正面的坑是怎么形成呢']})\n",
    "response = model.generate_content(messages)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "print(response.text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yZuuVwcsoHS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726722783283,
     "user_tz": -480,
     "elapsed": 618,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "9a122268-6791-4fc8-8ea3-63b3f499975a",
    "ExecuteTime": {
     "end_time": "2024-09-19T10:53:13.780991Z",
     "start_time": "2024-09-19T10:53:13.766042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "月球表面的坑洞，也就是陨石坑，主要由以下几种方式形成：\n",
      "\n",
      "**1. 陨石撞击：** 这是月球表面坑洞最主要的形成原因。月球没有大气层，无法像地球一样阻挡来自太空的陨石，因此会直接受到各种大小的陨石撞击。\n",
      "\n",
      "* **撞击过程：** 陨石以高速撞击月球表面，巨大的能量会瞬间将岩石融化、汽化，并产生巨大的冲击波。\n",
      "* **坑洞形成：** 冲击波会在地表产生一个碗状凹陷，这就是我们看到的陨石坑。坑洞的形状、大小取决于撞击陨石的大小、速度和角度。\n",
      "\n",
      "**2. 火山活动：** 月球表面也曾经存在火山活动，火山喷发会形成一些坑洞。\n",
      "\n",
      "* **火山喷发：** 熔岩从地表喷发，形成火山锥，也可能形成火山坑。\n",
      "* **火山坑：** 熔岩喷发后，火山口可能塌陷，形成一个凹陷的坑洞。\n",
      "\n",
      "**3. 地质运动：** 月球内部的地质运动也会形成一些坑洞。\n",
      "\n",
      "* **地壳运动：** 月球内部的地壳运动会产生裂缝，并可能形成一些深坑。\n",
      "\n",
      "**4. 阳光照射：** 太阳辐射对月球表面的岩石也会造成侵蚀，也会形成一些小的坑洞。\n",
      "\n",
      "**值得注意的是：**\n",
      "\n",
      "* 月球表面坑洞的数量和分布可以反映月球的历史。\n",
      "* 通过研究这些坑洞，我们可以了解月球的形成和演化。\n",
      "* 由于月球没有大气层，这些坑洞能够保存得很完整，可以为我们提供许多关于太阳系早期历史的信息。\n",
      "\n",
      "希望以上解答能让你对月球表面的坑洞形成有更深入的了解！\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "messages.append(response.candidates[0].content)\n",
    "print(messages)"
   ],
   "metadata": {
    "id": "YxVs5QJps8uT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726722898776,
     "user_tz": -480,
     "elapsed": 3388,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-09-19T10:53:30.766464Z",
     "start_time": "2024-09-19T10:53:30.752511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'parts': ['你好，我是羚伊']}, parts {\n",
      "  text: \"你好，羚伊！很高兴认识你。 你今天过得怎么样？你想聊些什么呢？ \\n\"\n",
      "}\n",
      "role: \"model\"\n",
      ", {'role': 'user', 'parts': ['月亮正面的坑是怎么形成呢']}, parts {\n",
      "  text: \"月球表面的坑洞，也就是陨石坑，主要由以下几种方式形成：\\n\\n**1. 陨石撞击：** 这是月球表面坑洞最主要的形成原因。月球没有大气层，无法像地球一样阻挡来自太空的陨石，因此会直接受到各种大小的陨石撞击。\\n\\n* **撞击过程：** 陨石以高速撞击月球表面，巨大的能量会瞬间将岩石融化、汽化，并产生巨大的冲击波。\\n* **坑洞形成：** 冲击波会在地表产生一个碗状凹陷，这就是我们看到的陨石坑。坑洞的形状、大小取决于撞击陨石的大小、速度和角度。\\n\\n**2. 火山活动：** 月球表面也曾经存在火山活动，火山喷发会形成一些坑洞。\\n\\n* **火山喷发：** 熔岩从地表喷发，形成火山锥，也可能形成火山坑。\\n* **火山坑：** 熔岩喷发后，火山口可能塌陷，形成一个凹陷的坑洞。\\n\\n**3. 地质运动：** 月球内部的地质运动也会形成一些坑洞。\\n\\n* **地壳运动：** 月球内部的地壳运动会产生裂缝，并可能形成一些深坑。\\n\\n**4. 阳光照射：** 太阳辐射对月球表面的岩石也会造成侵蚀，也会形成一些小的坑洞。\\n\\n**值得注意的是：**\\n\\n* 月球表面坑洞的数量和分布可以反映月球的历史。\\n* 通过研究这些坑洞，我们可以了解月球的形成和演化。\\n* 由于月球没有大气层，这些坑洞能够保存得很完整，可以为我们提供许多关于太阳系早期历史的信息。\\n\\n希望以上解答能让你对月球表面的坑洞形成有更深入的了解！\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T10:54:01.700699Z",
     "start_time": "2024-09-19T10:54:00.886639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages.append({'role': 'user', 'parts': ['我的名字叫什么？我的上一个问题是什么']})\n",
    "response = model.generate_content(messages)\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的名字叫 **羚伊**。\n",
      "\n",
      "你上一个问题是 **“月亮正面的坑是怎么形成呢”**。 \n",
      "\n",
      " 很高兴能记住你的名字和之前的对话！ 还有什么问题想问我吗？ \n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dpvHnM6tHsh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726722902282,
     "user_tz": -480,
     "elapsed": 597,
     "user": {
      "displayName": "Hart Diana",
      "userId": "03721461243608526835"
     }
    },
    "outputId": "af12971b-e002-4a51-cdff-16a5836bda9e"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "* 你的名字叫 **羚伊**。\n",
      "* 你的上一个问题是：**月亮正面的坑是怎么形成呢**\n",
      "\n",
      "\n",
      "很乐意为你服务！  有任何问题，尽管提出来。 \n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "messages = [{'role': 'user', 'parts': ['你好，我是羚伊']}]\n",
    "response = model.generate_content(messages)  # 你好，羚伊！很高兴认识你。 有什么可以帮到你的吗？ \n",
    "messages.append(response.candidates[0].content)\n",
    "messages.append({'role': 'user', 'parts': ['月亮正面的坑是怎么形成呢']})\n",
    "response = model.generate_content(messages)"
   ],
   "metadata": {
    "id": "_dH96OLNgeXr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "多轮对话机器人"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T08:27:57.902919Z",
     "start_time": "2024-09-23T08:27:57.890958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_with_Gemini(model):\n",
    "    conversation = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"你: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"结束对话。\")\n",
    "            print(conversation)\n",
    "            break\n",
    "        print(f\"你: {user_input}\")\n",
    "        # 添加用户输入到对话历史\n",
    "        conversation.append({\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [{\"text\": user_input}]\n",
    "        })\n",
    "\n",
    "        # 生成响应\n",
    "        response = model.generate_content(conversation)\n",
    "\n",
    "        # 打印模型响应\n",
    "        print(f\"Gemini: {response.text}\")\n",
    "\n",
    "        # 添加模型响应到对话历史\n",
    "        conversation.append({\n",
    "            \"role\": \"model\",\n",
    "            \"parts\": [{\"text\": response.text}]  # 将 response.text 作为 text 的值\n",
    "        })\n",
    "\n",
    "    return conversation"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T08:28:33.292316Z",
     "start_time": "2024-09-23T08:27:58.961506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"], transport='rest')\n",
    "model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
    "history_message = chat_with_Gemini(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你: 你好\n",
      "Gemini: 你好！ 很高兴你来找我，请问你想聊些什么呢？ \n",
      "\n",
      "你: 给小孩讲一个笑话‘\n",
      "Gemini: 为什么小兔子不喜欢玩捉迷藏？\n",
      "\n",
      "因为牠很容易被发现！ \n",
      "\n",
      "哈哈，你喜欢吗？ 还有其他想听的笑话吗？ \n",
      "\n",
      "你: 嗨\n",
      "Gemini: 嗨！ 很高兴你来找我，你想聊些什么呢？ 你想听个笑话吗？ \n",
      "\n",
      "结束对话。\n",
      "[{'role': 'user', 'parts': [{'text': '你好'}]}, {'role': 'model', 'parts': [{'text': '你好！ 很高兴你来找我，请问你想聊些什么呢？ \\n'}]}, {'role': 'user', 'parts': [{'text': '给小孩讲一个笑话‘'}]}, {'role': 'model', 'parts': [{'text': '为什么小兔子不喜欢玩捉迷藏？\\n\\n因为牠很容易被发现！ \\n\\n哈哈，你喜欢吗？ 还有其他想听的笑话吗？ \\n'}]}, {'role': 'user', 'parts': [{'text': '嗨'}]}, {'role': 'model', 'parts': [{'text': '嗨！ 很高兴你来找我，你想聊些什么呢？ 你想听个笑话吗？ \\n'}]}]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T08:28:39.452383Z",
     "start_time": "2024-09-23T08:28:39.438513Z"
    }
   },
   "cell_type": "code",
   "source": "print(history_message)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'parts': [{'text': '你好'}]}, {'role': 'model', 'parts': [{'text': '你好！ 很高兴你来找我，请问你想聊些什么呢？ \\n'}]}, {'role': 'user', 'parts': [{'text': '给小孩讲一个笑话‘'}]}, {'role': 'model', 'parts': [{'text': '为什么小兔子不喜欢玩捉迷藏？\\n\\n因为牠很容易被发现！ \\n\\n哈哈，你喜欢吗？ 还有其他想听的笑话吗？ \\n'}]}, {'role': 'user', 'parts': [{'text': '嗨'}]}, {'role': 'model', 'parts': [{'text': '嗨！ 很高兴你来找我，你想聊些什么呢？ 你想听个笑话吗？ \\n'}]}]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "之前的课程里我们使用了原生函数generate_content来在发送的prompt中通过包含历史消息的方式来实现多轮对话\n",
    "\n",
    "这次我们将要介绍新的专门为多轮对话准备的函数start chat"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T09:07:28.239965Z",
     "start_time": "2024-09-25T09:07:28.230961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"], transport='rest')\n",
    "model = genai.GenerativeModel('models/gemini-1.5-flash-exp-0827')"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T05:51:13.384934Z",
     "start_time": "2024-09-25T05:51:12.278805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = model.generate_content([{'role': 'user', 'parts': [{'text': '你好，我是羚伊'}]}, \n",
    "   {'role': 'model', 'parts': [{'text': '你好！ 很高兴你来找我，请问你想聊些什么呢？ \\n'}]},\n",
    "   {'role': 'user', 'parts': [{'text': '我上一句说了什么'}]}, \n",
    "   ])\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你上一句说的是：\n",
      "\n",
      "\n",
      "**“你好，我是羚伊”**\n",
      "\n",
      "\n",
      "很高兴认识你，羚伊！ \n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "可以简化格式成："
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T06:00:12.592457Z",
     "start_time": "2024-09-25T06:00:10.460208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = model.generate_content([{'role': 'user', 'parts': '你好，我是羚伊'}, \n",
    "   {'role': 'model', 'parts': '你好！ 很高兴你来找我，请问你想聊些什么呢？ \\n'},\n",
    "   {'role': 'user', 'parts': '我上一句说了什么'}, \n",
    "   ])\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你上一句说的是：\n",
      "\n",
      "\n",
      "**“你好，我是羚伊”** \n",
      "\n",
      "\n",
      "很高兴认识你，羚伊！ \n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T09:10:20.054767Z",
     "start_time": "2024-09-25T09:10:18.754228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_message = [{'parts': '你好，我是羚伊'}, \n",
    "                   {'parts': '你好！ 很高兴你来找我，请问你想聊些什么呢？ \\n'},\n",
    "                   {'parts': '我上一句说了什么'}, \n",
    "                   ]\n",
    "response = model.generate_content(history_message)\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?%24alt=json%3Benum-encoding%3Dint: Please use a valid role: user, model.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBadRequest\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32mD:\\Temp\\ipykernel_165912\\2736450108.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m                    \u001B[1;33m{\u001B[0m\u001B[1;34m'parts'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'我上一句说了什么'\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m                    ]\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerate_content\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhistory_message\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\generativeai\\generative_models.py\u001B[0m in \u001B[0;36mgenerate_content\u001B[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001B[0m\n\u001B[0;32m    329\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mgeneration_types\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mGenerateContentResponse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 331\u001B[1;33m                 response = self._client.generate_content(\n\u001B[0m\u001B[0;32m    332\u001B[0m                     \u001B[0mrequest\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    333\u001B[0m                     \u001B[1;33m**\u001B[0m\u001B[0mrequest_options\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\u001B[0m in \u001B[0;36mgenerate_content\u001B[1;34m(self, request, model, contents, retry, timeout, metadata)\u001B[0m\n\u001B[0;32m    828\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    829\u001B[0m         \u001B[1;31m# Send the request.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 830\u001B[1;33m         response = rpc(\n\u001B[0m\u001B[0;32m    831\u001B[0m             \u001B[0mrequest\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    832\u001B[0m             \u001B[0mretry\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mretry\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001B[0m\n\u001B[0;32m    129\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"compression\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompression\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 131\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\u001B[0m in \u001B[0;36mretry_wrapped_func\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    291\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_initial\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maximum\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmultiplier\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_multiplier\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    292\u001B[0m             )\n\u001B[1;32m--> 293\u001B[1;33m             return retry_target(\n\u001B[0m\u001B[0;32m    294\u001B[0m                 \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    295\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_predicate\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\u001B[0m in \u001B[0;36mretry_target\u001B[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mexc\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m             \u001B[1;31m# defer to shared logic for handling errors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 153\u001B[1;33m             _retry_error_helper(\n\u001B[0m\u001B[0;32m    154\u001B[0m                 \u001B[0mexc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m                 \u001B[0mdeadline\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py\u001B[0m in \u001B[0;36m_retry_error_helper\u001B[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001B[0m\n\u001B[0;32m    210\u001B[0m             \u001B[0moriginal_timeout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    211\u001B[0m         )\n\u001B[1;32m--> 212\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mfinal_exc\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msource_exc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    213\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mon_error_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    214\u001B[0m         \u001B[0mon_error_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\u001B[0m in \u001B[0;36mretry_target\u001B[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[0m\n\u001B[0;32m    142\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0msleep\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msleep_generator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 144\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misawaitable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    146\u001B[0m                 \u001B[0mwarnings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_ASYNC_RETRY_WARNING\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\api_core\\timeout.py\u001B[0m in \u001B[0;36mfunc_with_timeout\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    118\u001B[0m                 \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"timeout\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_timeout\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mtime_since_first_attempt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 120\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    121\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    122\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mfunc_with_timeout\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\api_core\\grpc_helpers.py\u001B[0m in \u001B[0;36merror_remapped_callable\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0merror_remapped_callable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 76\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mcallable_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     77\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mgrpc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mRpcError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mexc\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     78\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mexceptions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_grpc_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mexc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\MajorSoft\\Anaconda3\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\rest.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, request, retry, timeout, metadata)\u001B[0m\n\u001B[0;32m    845\u001B[0m             \u001B[1;31m# subclass.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    846\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatus_code\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m400\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 847\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mcore_exceptions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_http_response\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    848\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    849\u001B[0m             \u001B[1;31m# Return the response\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mBadRequest\u001B[0m: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-exp-0827:generateContent?%24alt=json%3Benum-encoding%3Dint: Please use a valid role: user, model."
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "技巧：预填写"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T05:52:53.417323Z",
     "start_time": "2024-09-25T05:52:50.954341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "response = model.generate_content([{'role': 'user', 'parts': [{'text': '你好，我是羚伊'}]}, \n",
    "{'role': 'model', 'parts': [{'text': '你好！ 很高兴你来找我，请问你想聊些什么呢？ \\n'}]},\n",
    "{'role': 'user', 'parts': [{'text': '我上一句说了什么'}]},\n",
    "{'role': 'model', 'parts': [{'text': '你说的是“你好'}]},\n",
    "])\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，我是羚伊”。\n",
      "\n",
      "\n",
      "很高兴认识你，羚伊！ \n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "接下来我们要介绍Gemini封装的为多轮对话设计的函数start_chat()"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T09:14:49.633490Z",
     "start_time": "2024-09-25T09:14:42.847886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat = model.start_chat()\n",
    "response = chat.send_message(\"你好，我是羚伊\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"科学研究理论上人最多能有多少个亲密社交关系，还有什么相关的冷知识吗\")\n",
    "print(response.text)\n",
    "response = chat.send_message(\"我的名字是什么\")\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，羚伊！很高兴认识你。 \n",
      "\n",
      "有什么我可以帮助你的吗？ \n",
      "\n",
      "## 人最多能有多少个亲密社交关系？\n",
      "\n",
      "理论上，人最多能维持的稳定亲密社交关系数量，被认为是**150人左右**，这个数字被称为 **“邓巴数字”** (Dunbar's number)。 \n",
      "\n",
      "**邓巴数字的由来：**\n",
      "\n",
      "* 英国人类学家罗宾·邓巴（Robin Dunbar）通过对灵长类动物的研究发现，灵长类动物的群体规模与其大脑新皮层的体积成正比。\n",
      "* 他将这种关系应用到人类身上，推测出人类大脑新皮层的大小能够支持大约150个稳定关系的认知负荷。\n",
      "\n",
      "**需要注意的是，这个数字只是一个估计，并非绝对的限制。** \n",
      "\n",
      "* **“亲密社交关系”的定义:** 邓巴数字指的是能够记住名字、了解其基本情况、并在社交场合中进行互动的人群。并非所有150人都需要是最好的朋友，而是指在一个社交网络中能够保持联系的个体。\n",
      "* **个体差异:**  每个人的社交能力和需求不同，有些人可能拥有更广泛的社交圈，而有些人则更偏向于小而紧密的社交圈。\n",
      "* **社会网络的影响:** 如今的社交媒体和互联网使得我们能够与更多人保持联系，但这并不一定意味着这些关系都属于“亲密社交关系”。\n",
      "\n",
      "## 相关的冷知识：\n",
      "\n",
      "* **“150法则”在实际生活中也有体现:** 许多组织和团体，例如早期教会、军队和部落，其规模通常都维持在150人左右，这可能与人类的社交认知能力有关。\n",
      "* **人类的社交网络并非孤立的：** 我们的社交网络通常会形成层级结构，核心层包含最亲密的朋友和家人，外围层则包含较为疏远的熟人。\n",
      "* **社交关系对心理健康至关重要:** 拥有稳定的社交关系能够促进心理健康，而社交孤立则可能导致抑郁和焦虑等心理问题。\n",
      "* **现代社会对社交关系提出了挑战:** 快节奏的生活和数字化的沟通方式，可能导致我们与亲朋好友的联系减少，从而影响社交关系的质量。\n",
      "\n",
      "\n",
      "希望以上信息能够帮助你了解人类社交关系的奥秘！如果你还有其他问题，欢迎随时提问。 \n",
      "\n",
      "你的名字是**羚伊**。 \n",
      "\n",
      "我记得我们一开始的对话，你就是这么称呼自己的。 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240923185542.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "可以看到，开启的对话实际上是一个ChatSession对象，我们来看看它的架构"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240925144604.png)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. ChatSession 的作用:\n",
    "   这个类用于管理与 AI 模型的持续对话。它保存了整个对话的历史记录。\n",
    "\n",
    "2. 主要属性:\n",
    "   - history: 保存整个对话的历史记录\n",
    "   - last: 返回最后一次收到的 AI 回复\n",
    "\n",
    "3. 主要方法:\n",
    "   - __init__: 初始化聊天会话,可以提供一个初始的对话历史\n",
    "   - send_message: 发送消息给 AI 并获取回复\n",
    "     - 可以设置生成配置、安全设置等\n",
    "     - 支持流式响应(stream=True)\n",
    "   - send_message_async: send_message 的异步版本\n",
    "   - rewind: 从对话历史中删除最后一对请求/响应\n",
    "\n",
    "4. 使用示例:\n",
    "   ```python\n",
    "   model = genai.GenerativeModel('models/gemini-pro')\n",
    "   chat = model.start_chat()\n",
    "   response = chat.send_message(\"Hello\")\n",
    "   print(response.text)\n",
    "   ```\n",
    "\n",
    "5. 特点:\n",
    "   - 自动保存对话历史\n",
    "   - 可以设置生成参数和安全设置\n",
    "   - 支持流式响应,可以边生成边输出\n",
    "   - 提供同步和异步两种调用方式\n",
    "   - 可以通过 history 属性查看完整对话历史\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T09:23:12.345579Z",
     "start_time": "2024-09-25T09:21:46.597552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"], transport='rest')\n",
    "chat = model.start_chat()\n",
    "r1 = chat.send_message(\"我是羚伊\")\n",
    "print(r1.text)\n",
    "r2 = chat.send_message(\"我的名字是什么？\")\n",
    "print(r2.text)\n",
    "r3 = chat.send_message(\"上一个问题是什么\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "很高興認識您，羚伊！請問您想聊些什麼呢？ \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mD:\\Temp\\ipykernel_165912\\1511747179.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mr2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mchat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_message\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"我的名字是什么？\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[0mr3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mchat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_message\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"上一个问题是什么\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Temp\\ipykernel_165912\\1511747179.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mr2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mchat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_message\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"我的名字是什么？\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[0mr3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mchat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_message\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"上一个问题是什么\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mD:\\PyCharm 2024.1.4\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1199\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1200\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1201\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1202\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1203\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\PyCharm 2024.1.4\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1214\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1215\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1216\u001B[1;33m                 \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.01\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1217\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1218\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "使用start_chat的send_message函数和generate_content原生的对话函数做对比："
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "入参："
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240923164109.png)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T09:26:02.048523Z",
     "start_time": "2024-09-25T09:26:02.031523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def start_chat_with_Gemini(model):\n",
    "    chat = model.start_chat()\n",
    "    while True:\n",
    "        user_input = input('用户: ')\n",
    "        if user_input.lower() == \"exit\":\n",
    "                print(\"结束对话。\")\n",
    "                break\n",
    "        chat.send_message(user_input)\n",
    "        print(f\"Assistant:{chat.last.text}\")\n",
    "    return chat.history"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T09:27:14.181222Z",
     "start_time": "2024-09-25T09:27:14.161221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def orgin_chat_with_Gemini(model):\n",
    "    conversation = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"你: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"结束对话。\")\n",
    "            break\n",
    "        print(f\"你: {user_input}\")\n",
    "        # 添加用户输入到对话历史\n",
    "        conversation.append({\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [{\"text\": user_input}]\n",
    "        })\n",
    "\n",
    "        # 生成响应\n",
    "        response = model.generate_content(conversation)\n",
    "\n",
    "        # 打印模型响应\n",
    "        print(f\"Gemini: {response.text}\")\n",
    "\n",
    "        # 添加模型响应到对话历史\n",
    "        conversation.append({\n",
    "            \"role\": \"model\",\n",
    "            \"parts\": [{\"text\": response.text}]  # 将 response.text 作为 text 的值\n",
    "        })\n",
    "\n",
    "    return conversation"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T09:27:34.083494Z",
     "start_time": "2024-09-25T09:27:16.486756Z"
    }
   },
   "cell_type": "code",
   "source": "orgin_history=orgin_chat_with_Gemini(model)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你: hi 我是羚伊\n",
      "Gemini: 您好，羚伊！很高兴认识你！ 你想聊些什么呢？ \n",
      "\n",
      "你: 我的上一个问题是什么\n",
      "Gemini: 你的上一个问题是： \"hi 我是羚伊\"\n",
      "\n",
      "你想继续聊些什么呢？ \n",
      "\n",
      "结束对话。\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T09:26:31.671641Z",
     "start_time": "2024-09-25T09:26:08.264565Z"
    }
   },
   "cell_type": "code",
   "source": "start_chat_history=start_chat_with_Gemini(model)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:你好，羚伊！很高兴认识你 你想聊些什么呢？ \n",
      "\n",
      "Assistant:你上一个问题是：\n",
      "\n",
      "**\"hi 我是羚伊\"**\n",
      "\n",
      "你好像刚开始跟我聊天呢！ \n",
      "\n",
      "你想聊些什么呢？我可以陪你聊聊最近的新闻，或者推荐你看什么电影，也可以跟你聊聊你的爱好！ \n",
      "\n",
      "结束对话。\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chat = model.start_chat(start_chat_history)\n",
    "response=chat.send_message(\"我都问过什么问题\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://typora-photo1220.oss-cn-beijing.aliyuncs.com/DataAnalysis/LingYi/20240925150057.png)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T11:47:58.042876Z",
     "start_time": "2024-09-30T11:47:43.621706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import google.generativeai as genai\n",
    "\n",
    "def enhanced_chat_with_gemini(model, initial_history=None, max_turns=10, save_history=True):\n",
    "    \"\"\"\n",
    "    Enhanced Gemini chat function.\n",
    "\n",
    "    :param model: Gemini model instance\n",
    "    :param initial_history: Initial chat history (optional)\n",
    "    :param max_turns: Maximum number of chat turns\n",
    "    :param save_history: Whether to save the chat history\n",
    "    :return: Complete chat history\n",
    "    \"\"\"\n",
    "    # Initialize chat\n",
    "    try:\n",
    "        if initial_history:\n",
    "            chat = model.start_chat(history=initial_history)\n",
    "            print(\"Loaded previous chat history.\")\n",
    "        else:\n",
    "            chat = model.start_chat()\n",
    "            print(\"Starting a new chat.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing chat: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    turn_count = 0\n",
    "    while turn_count < max_turns:\n",
    "        # User input\n",
    "        user_input = input('User: ')\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Ending chat.\")\n",
    "            break\n",
    "\n",
    "        # Handle special commands\n",
    "        if user_input.startswith(\"/\"):\n",
    "            handle_special_command(user_input, chat)\n",
    "            continue\n",
    "\n",
    "        # Send message to model\n",
    "        try:\n",
    "            response = chat.send_message(user_input)\n",
    "            print(f\"AI: {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        turn_count += 1\n",
    "\n",
    "    if save_history:\n",
    "        save_chat_history(chat.history)\n",
    "\n",
    "    return chat.history\n",
    "\n",
    "def handle_special_command(command, chat):\n",
    "    \"\"\"Handle special commands.\"\"\"\n",
    "    if command == \"/history\":\n",
    "        print(\"\\nChat History:\")\n",
    "        for msg in chat.history:\n",
    "            role = \"User\" if msg.role == \"user\" else \"AI\"\n",
    "            content = ''.join([part.text for part in msg.parts])\n",
    "            print(f\"{role}: {content}\")\n",
    "    elif command == \"/clear\":\n",
    "        chat.history = []\n",
    "        print(\"Chat history has been cleared.\")\n",
    "    elif command == \"/save\":\n",
    "        save_chat_history(chat.history)\n",
    "    else:\n",
    "        print(\"Unknown command. Available commands: /history, /clear, /save\")\n",
    "\n",
    "def save_chat_history(chat_history):\n",
    "    \"\"\"Save chat history to a JSON file.\"\"\"\n",
    "    # Convert chat history to a serializable format\n",
    "    serializable_history = []\n",
    "    for msg in chat_history:\n",
    "        try:\n",
    "            # 提取消息内容\n",
    "            content = ''\n",
    "            for part in msg.parts:\n",
    "                content += part.text\n",
    "            # 创建可序列化的字典\n",
    "            serializable_msg = {\n",
    "                'role': msg.role,\n",
    "                'parts': content\n",
    "            }\n",
    "            serializable_history.append(serializable_msg)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing message: {e}\")\n",
    "            continue  # 跳过无法处理的消息\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"chat_history_{timestamp}.json\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(serializable_history, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"对话历史已保存到 {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving chat history: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = os.environ.get(\"API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"API_KEY environment variable not set.\")\n",
    "        exit(1)\n",
    "\n",
    "    genai.configure(api_key=api_key, transport='rest')\n",
    "    model = genai.GenerativeModel('models/gemini-1.5-flash-exp-0827')\n",
    "\n",
    "    # Optionally load previous chat history\n",
    "    with open('chat_history_20240930_194511.json', 'r', encoding='utf-8') as f:\n",
    "        initial_history = json.load(f)\n",
    "    enhanced_chat_with_gemini(model, initial_history=initial_history)\n",
    "# \n",
    "#     # Or start a new chat\n",
    "#     enhanced_chat_with_gemini(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previous chat history.\n",
      "AI: 你叫**羚伊**。 \n",
      "\n",
      "I remember! You told me earlier.  \n",
      "\n",
      "Ending chat.\n",
      "对话历史已保存到 chat_history_20240930_194758.json\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T11:47:40.233547Z",
     "start_time": "2024-09-30T11:47:40.226570Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
